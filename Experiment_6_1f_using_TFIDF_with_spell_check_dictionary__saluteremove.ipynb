{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment 6.1f- using TFIDF with spell_check dictionary _saluteremove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1P9EYNZHT17tVxh0rqG75yxNplhJzW8yN",
      "authorship_tag": "ABX9TyO0H5vzetjaKgsNL1ABNAL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npd2013/Mypublicmodels/blob/main/Experiment_6_1f_using_TFIDF_with_spell_check_dictionary__saluteremove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2jwlnJOe0wa"
      },
      "source": [
        "SETUP1:-  \n",
        "-Single document text summarization \n",
        "\n",
        "-frequency based method for scoring sentences \n",
        "\n",
        "-preprocessing applied are knowledge based normalization,  stopword removal and stemming.\n",
        "-NO CUE WORDS \n",
        "\n",
        "-automatic evaluation using rouge and compression ratio analysis.\n",
        "\n",
        "note:results stored in another file \n",
        "\n",
        "\n",
        "Algorithm steps :- \n",
        "\n",
        "1) Preprocessing of text- \n",
        "\n",
        "    1.1)Cleaning \n",
        "\n",
        "    1.1.1)Removal of unnecessary symbols/charachters like html tags ,extra spaces,extra newlines etc.\n",
        "         \n",
        " \n",
        " 1.2) Line segmentation\n",
        " \n",
        "    -using nltk-punkt \n",
        "    -Divide text into lines.\n",
        " 1.3) word segmentation\n",
        "    -Divide the lines into words.\n",
        "    -words can be separated by whitespace or characters like quotes or comma.  use the punctuation list to exclude from the word.Only extract valid words \n",
        "\n",
        "  1.4) Stopword removal \n",
        "       -basic set of 15 words removed from lines. \n",
        "   \n",
        " 1.5) Stemming\n",
        "  use indowordnet as dictionary of standard words and prefix,suffix AND replacement longest applicable rules for finding stem of words.Also if by trimming size of word becomes less than 3, it is considered to be overstemmed ,so ignored.  \n",
        "                   \n",
        "               \n",
        "       \n",
        "\n",
        "2)Make vector space representation -based on term frequency\n",
        "\n",
        "    2.1) Compute tfidf using sklearn module of python.\n",
        "\n",
        "    2.2) For each sentence - \n",
        "\n",
        "     2.2.1) Compute sentence score= total tfidf score of all words in sentence divide by number of words in sentence.\n",
        "     2.2.2) If it is first or last sentence , increase its weightage ten times.\n",
        "  \n",
        "  2.3) For each sentence -\n",
        "\n",
        "     3.2.1) Compute the average normalized score  : \n",
        "          Current sentence score divide by total sentence score of text.\n",
        "      \n",
        "\n",
        " 4) //Find top \"n\" scored sentences.\n",
        "\n",
        " 4.1) Set n= number of lines in gold summary. 0r \"x\" percent of original text.    \n",
        " 4.2) Summary_length=n\n",
        " 4.3) Sorted_sentences=sort(sentences) based on their scores.\n",
        "    \n",
        " 5) //Generate the summary -\n",
        "\n",
        "    for sentences = 1 to gold_summary_length \n",
        "      - if sentence in gold_summary  \n",
        "           put sentence in system_summary. \n",
        "  \n",
        "6)// Evaluate the summary-\n",
        "\n",
        "    6.1) use the Rouge score \n",
        "      - Get the precision ,recall and F-measure for rouge-1 ,rouge2 and rouge-LCS.   \n",
        "\n",
        "    6.2) Find comression ratio of text\n",
        "       6.2.1) sentence compression ratio\n",
        "        = total sentences in system_summary/total sentences in source text \n",
        "        6.2.2) Word compression ratio\n",
        "        =total words in system_summary/total words in source \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeLGOonUpR4z",
        "outputId": "5f2849f9-1590-4223-81d4-abad7c2a6956"
      },
      "source": [
        "#install needed packages \n",
        "#!pip install  gensim  #has word2vec model built, using english \n",
        "\n",
        "!pip install pyiwn\n",
        "!pip install -U scikit-learn\n",
        "!pip install rouge\n",
        "!pip install indic-nlp-library\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyiwn in /usr/local/lib/python3.6/dist-packages (0.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pyiwn) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyiwn) (1.1.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyiwn) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->pyiwn) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyiwn) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->pyiwn) (1.15.0)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.24.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.6/dist-packages (0.71)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7gUXWZMwb6D"
      },
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/guj_final_preporcess-v6.py\" \"guj_final_preporcess.py\" #The preprocessing modules designed\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/Evaluation.py\" \"Evaluation.py\"   # the evaluation module designed\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/guj_final_postprocessing-v1.py\" \"post.py\"   # Get the post processing module \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo4oVUa5g9D-"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import io\n",
        "import csv\n",
        "from collections import Counter\n",
        "from Evaluation import *\n",
        "from post import *\n",
        "import pandas as pd \n",
        "from guj_final_preporcess import * #to get the required preporcessing modules -from self made pipeline\n",
        "import pyiwn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7XlBVjZMvCPA",
        "outputId": "2dd6029b-771f-492c-d52f-bbfae2634a3c"
      },
      "source": [
        "from google.colab import files #upload the files from local machine on the colab environment \n",
        "uploaded = files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a1733ce7-e554-4771-af66-b56b934e9c33\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a1733ce7-e554-4771-af66-b56b934e9c33\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving story1.txt to story1 (1).txt\n",
            "Saving story1-gold.txt to story1-gold (1).txt\n",
            "Saving story1-topic.txt to story1-topic.txt\n",
            "Saving story2.txt to story2.txt\n",
            "Saving story2-gold.txt to story2-gold.txt\n",
            "Saving story2-topic.txt to story2-topic.txt\n",
            "Saving story3.txt to story3.txt\n",
            "Saving story3-gold.txt to story3-gold.txt\n",
            "Saving story3-topic.txt to story3-topic.txt\n",
            "Saving story4.txt to story4.txt\n",
            "Saving story4-gold.txt to story4-gold.txt\n",
            "Saving story4-topic.txt to story4-topic.txt\n",
            "Saving story5.txt to story5.txt\n",
            "Saving story5-gold.txt to story5-gold.txt\n",
            "Saving story5-topic.txt to story5-topic.txt\n",
            "Saving story6.txt to story6.txt\n",
            "Saving story6-gold.txt to story6-gold.txt\n",
            "Saving story6-topic.txt to story6-topic.txt\n",
            "Saving story7.txt to story7.txt\n",
            "Saving story7-gold.txt to story7-gold.txt\n",
            "Saving story7-topic.txt to story7-topic.txt\n",
            "Saving story8.txt to story8.txt\n",
            "Saving story8-gold.txt to story8-gold.txt\n",
            "Saving story8-topic.txt to story8-topic.txt\n",
            "Saving story9.txt to story9.txt\n",
            "Saving story9-gold.txt to story9-gold.txt\n",
            "Saving story9-topic.txt to story9-topic.txt\n",
            "Saving story10.txt to story10.txt\n",
            "Saving story10-gold.txt to story10-gold.txt\n",
            "Saving story10-topic.txt to story10-topic.txt\n",
            "Saving story11.txt to story11.txt\n",
            "Saving story11-gold.txt to story11-gold.txt\n",
            "Saving story11-topic.txt to story11-topic.txt\n",
            "Saving story12.txt to story12.txt\n",
            "Saving story12-gold.txt to story12-gold.txt\n",
            "Saving story12-topic.txt to story12-topic.txt\n",
            "Saving story13.txt to story13.txt\n",
            "Saving story13-gold.txt to story13-gold.txt\n",
            "Saving story13-topic.txt to story13-topic.txt\n",
            "Saving story14.txt to story14.txt\n",
            "Saving story14-gold.txt to story14-gold.txt\n",
            "Saving story14-topic.txt to story14-topic.txt\n",
            "Saving story15.txt to story15.txt\n",
            "Saving story15-gold.txt to story15-gold.txt\n",
            "Saving story15-topic.txt to story15-topic.txt\n",
            "Saving story16.txt to story16.txt\n",
            "Saving story16-gold.txt to story16-gold.txt\n",
            "Saving story16-topic.txt to story16-topic.txt\n",
            "Saving story17.txt to story17.txt\n",
            "Saving story17-gold.txt to story17-gold.txt\n",
            "Saving story17-topic.txt to story17-topic.txt\n",
            "Saving story18.txt to story18.txt\n",
            "Saving story18-gold.txt to story18-gold.txt\n",
            "Saving story18-topic.txt to story18-topic.txt\n",
            "Saving story19.txt to story19.txt\n",
            "Saving story19-gold.txt to story19-gold.txt\n",
            "Saving story19-topic.txt to story19-topic.txt\n",
            "Saving story20.txt to story20.txt\n",
            "Saving story20-gold.txt to story20-gold.txt\n",
            "Saving story20-topic.txt to story20-topic.txt\n",
            "Saving story21.txt to story21.txt\n",
            "Saving story21-gold.txt to story21-gold.txt\n",
            "Saving story21-topic.txt to story21-topic.txt\n",
            "Saving story22.txt to story22.txt\n",
            "Saving story22-gold.txt to story22-gold.txt\n",
            "Saving story22-topic.txt to story22-topic.txt\n",
            "Saving story23.txt to story23.txt\n",
            "Saving story23-gold.txt to story23-gold.txt\n",
            "Saving story23-topic.txt to story23-topic.txt\n",
            "Saving story24.txt to story24.txt\n",
            "Saving story24-gold.txt to story24-gold.txt\n",
            "Saving story24-topic.txt to story24-topic.txt\n",
            "Saving story25.txt to story25.txt\n",
            "Saving story25-gold.txt to story25-gold.txt\n",
            "Saving story25-topic.txt to story25-topic.txt\n",
            "Saving story26.txt to story26.txt\n",
            "Saving story26-gold.txt to story26-gold.txt\n",
            "Saving story26-topic.txt to story26-topic.txt\n",
            "Saving story27.txt to story27.txt\n",
            "Saving story27-gold.txt to story27-gold.txt\n",
            "Saving story27-topic.txt to story27-topic.txt\n",
            "Saving story28.txt to story28.txt\n",
            "Saving story28-gold.txt to story28-gold.txt\n",
            "Saving story28-topic.txt to story28-topic.txt\n",
            "Saving story29.txt to story29.txt\n",
            "Saving story29-gold.txt to story29-gold.txt\n",
            "Saving story29-topic.txt to story29-topic.txt\n",
            "Saving story30.txt to story30.txt\n",
            "Saving story30-gold.txt to story30-gold.txt\n",
            "Saving story30-topic.txt to story30-topic.txt\n",
            "Saving story31.txt to story31.txt\n",
            "Saving story31-gold.txt to story31-gold.txt\n",
            "Saving story31-topic.txt to story31-topic.txt\n",
            "Saving story32.txt to story32.txt\n",
            "Saving story32-gold.txt to story32-gold.txt\n",
            "Saving story32-topic.txt to story32-topic.txt\n",
            "Saving story33.txt to story33.txt\n",
            "Saving story33-gold.txt to story33-gold.txt\n",
            "Saving story33-topic.txt to story33-topic.txt\n",
            "Saving story34.txt to story34.txt\n",
            "Saving story34-gold.txt to story34-gold.txt\n",
            "Saving story34-topic.txt to story34-topic.txt\n",
            "Saving story35.txt to story35.txt\n",
            "Saving story35-gold.txt to story35-gold.txt\n",
            "Saving story35-topic.txt to story35-topic.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu3f_IRUd2NW",
        "outputId": "4f114031-2c6f-4a46-c7c7-b1952593d709"
      },
      "source": [
        "\r\n",
        "#take all the needed inputs from user \r\n",
        "basename=input('Enter the basic name of file(without txt),other names wud be autogenerated  ')\r\n",
        "filename=basename+'.txt'\r\n",
        "gold_filename=basename+'-gold.txt'\r\n",
        "#filename=input('Enter text file name to be summarized ') #Ex: file1.txt\r\n",
        "#analysefile_name=filename.replace('.txt','analysis-TF+ALL.txt')# to store the data for later analysis . Example- file1-results.txt \r\n",
        "#features_filename=filename.replace('.txt','TF+ALL-features.csv') # file-1-features.csv\r\n",
        "#gold_filename=input(\"Enter name of file containing gold summary  \") #example file1-gold.txt ,assumes the name is having pattern ending with gold.txt \r\n",
        "summary_filename=filename.replace('gold.txt','-TF+ALL_systemsummary-topic based.txt') # example file1-gold-system-summary.txt\r\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the basic name of file(without txt),other names wud be autogenerated  story35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_1DqGJHdBBr",
        "outputId": "755c43b8-c61e-4c0f-b0a3-653097320c0e"
      },
      "source": [
        "#read the input original file . \r\n",
        "\r\n",
        "\r\n",
        "guj_clean_html(filename,filename)# text cleaning remove the html tags and overwrite the original file \r\n",
        "guj_clean_html(gold_filename,gold_filename)# clean the html tags from gold file \r\n",
        "\r\n",
        "#Display the given data in input files \r\n",
        "with io.open(filename,'r',encoding='utf-8') as f: \r\n",
        "\t\t  lines=f.read()#get all the lines from file\r\n",
        "analyzedfile_obj=open(analysefile_name,mode='w',encoding='UTF-8')\r\n",
        "analyzedfile_obj.write(\"\\n\\n The original file is :------\\n\")\r\n",
        "analyzedfile_obj.write(lines)\r\n",
        "\r\n",
        "with io.open(gold_filename,'r',encoding='utf-8') as f: \r\n",
        "\t\t  lines=f.read()#get all the lines from file\r\n",
        "\r\n",
        "analyzedfile_obj.write(\"\\n\\n The gold summary provided by expert  is -->\\n\")    \r\n",
        "analyzedfile_obj.write(lines)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# now find the statistics of originally given informations \r\n",
        "originalcorpora,totaloriginalsentences=guj_corpus_generate(filename) #make corpus of original data \r\n",
        "goldcorpora,totalgoldcorpora=guj_corpus_generate(gold_filename) #make corpus of gold \r\n",
        "\r\n",
        "analyzedfile_obj.write(\"\\n\\n STATISTICS------>\")\r\n",
        "analyzedfile_obj.writelines(\"\\n Original file sentences(RAW) - {0}\\n\".format(totaloriginalsentences))\r\n",
        "analyzedfile_obj.writelines(\"\\n gold file sentences(RAW)- {0}\\n\".format(totalgoldcorpora))\r\n",
        "\r\n",
        "###################################################################\r\n",
        "########################  Step-1 ##################################\r\n",
        "#####################  cleaning phase  ########################\r\n",
        "\r\n",
        "#doing cleaning of file \r\n",
        "#remove newlines and other unnecessary data and clean it for feature extraction \r\n",
        "#to replace the data as per our format, replacing abbreviations dot with '*'\r\n",
        "normfile=\"temp1.txt\"\r\n",
        "norm_gold_file=\"temp2.txt\"\r\n",
        "\r\n",
        "guj_clean_text(filename,normfile) \r\n",
        "guj_clean_text(gold_filename,norm_gold_file)\r\n",
        "\r\n",
        "\r\n",
        "#Now display the stats and make corpora after cleaning file and removing new lines . \r\n",
        "normcorpora,totalnormsentences=guj_corpus_generate(normfile)  \r\n",
        "goldcorpora,totalnormgoldsentences=guj_corpus_generate(norm_gold_file) \r\n",
        "\r\n",
        "\r\n",
        "analyzedfile_obj.write(\" Segmented Lines in source -{0} \\n \".format(totalnormsentences))\r\n",
        "analyzedfile_obj.write(\" Segmented Lines in gold -{0}\\n \".format(totalnormgoldsentences))\r\n",
        "\r\n",
        "###################################################################\r\n",
        "########################  Step-2 ##################################\r\n",
        "#####################  Normalization and tokenization  for feature extraction  ########################\r\n",
        "\r\n",
        "cleanfile=\"file.txt\"\r\n",
        "mydictionary=guj_makedictionary()\r\n",
        "guj_stopwordremoval(normfile,cleanfile) #remove stopwords\r\n",
        "guj_stemmer_new(cleanfile,cleanfile,mydictionary)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#####\r\n",
        "\r\n",
        "\r\n",
        "#Now working on the modified file to get various features \r\n",
        "\r\n",
        "corpora,totalsent=guj_corpus_generate(cleanfile) #divide lines into sentences-\r\n",
        "analyzedfile_obj.write(\" Lines in source after all processing -{0} \\n \".format(totalsent))\r\n",
        "\r\n",
        "sent_pos_list=[] #to store the sentence position\r\n",
        "sent_words_count=[]  # to store the total words in each sentence \r\n",
        "sent_number=0\r\n",
        "for s in corpora:\r\n",
        "  sent_pos_list.append(sent_number)\r\n",
        "  sent_number+=1\r\n",
        " # print(\"\\nsentence is \",s)\r\n",
        "  tokens=guj_mytokenizer(s)\r\n",
        " # print(\"\\ntokens are \",tokens)\r\n",
        "  \r\n",
        "\r\n",
        "# to generate vector based on frequency \r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "vectorizer = TfidfVectorizer(tokenizer = guj_mytokenizer,min_df=0.15) #frequency of words with atleast 15% frequency\r\n",
        "\r\n",
        "cwm = vectorizer.fit_transform(corpora) #vectorize the corpus of input file\r\n",
        "\r\n",
        "tokens = vectorizer.get_feature_names() #get the names of words used in vector\r\n",
        "print(\"tokens / words extracted are \",tokens)\r\n",
        "counts=cwm.toarray().sum(axis=1)\r\n",
        "counts=cwm.sum(axis=0).A1\r\n",
        "freqwithwords=dict(zip(tokens,counts))\r\n",
        "analyzedfile_obj.write(\"\\n\\n****The TFIDF  based vector is formed ****\")\r\n",
        "analyzedfile_obj.write('\\n The dimension of vector matrix generated is -\\n {0} '.format(cwm.shape))\r\n",
        "analyzedfile_obj.write('\\n\\n Vocabulory of words with frequencies are  -  \\n {0} '.format(freqwithwords,2))\r\n",
        "#to store the dataset in csvfile\r\n",
        "print(\"Original freq values of words considered were \",freqwithwords)\r\n",
        " \r\n",
        " \r\n",
        "list_of_tuples = list(zip(sent_pos_list,corpora, cwm.toarray()))  \r\n",
        "#generate a dataframe to store the data \r\n",
        "import pandas as pd \r\n",
        "df=pd.DataFrame(list_of_tuples, columns=['sent_pos','sentence','frequency_vector'])\r\n",
        "df.to_csv(features_filename) \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "##########################################################\r\n",
        "################# ######Step3#############################\r\n",
        "################ Compute sentence scores #################\r\n",
        "##########################################################\r\n",
        "## Individual Sentence scoring \r\n",
        "\r\n",
        "\r\n",
        "score_list=[] #to store sentencewise scores\r\n",
        "\r\n",
        "total_allsent_scores=0\r\n",
        "for s in corpora:\r\n",
        "  words=0\r\n",
        "  sentenceval=0 #to store the score of sentence \r\n",
        "  \r\n",
        "  for word in guj_mytokenizer(s): #use own tokenizer\r\n",
        "    for key,value in freqwithwords.items():\r\n",
        "      if key ==word:\r\n",
        "        sentenceval=sentenceval+value\r\n",
        "        words=words+1 #important word in current sentence increased\r\n",
        "  sentenceval = float(f\"{sentenceval :.2f}\") #rounding off to two numbers\r\n",
        "  if words>0:\r\n",
        "    sentenceval=sentenceval/words #find the average score of sentence ,to normalize for long sentences\r\n",
        "  score_list.append(sentenceval)    #store the direct value of sentence based on words found in it   \r\n",
        "  total_allsent_scores=total_allsent_scores+sentenceval #update the total score of all corpus\r\n",
        "      \r\n",
        "avg_sent_score=total_allsent_scores/len(corpora) #find average score of sentence in corpus    \r\n",
        "newscorelist=[]\r\n",
        "\r\n",
        "pos=0 #position index in dataframes start from 0\r\n",
        "for score in score_list:\r\n",
        "  #score=score/avg_sent_score #normalize by average score \r\n",
        "  score=score/total_allsent_scores #normalize by total score ,gives better accuracy\r\n",
        "  sent_position=df.iat[pos,0] #fetch the sentence position ,to update special weight to first and last line \r\n",
        "  if sent_position==0 or sent_position==totalsent-1:#as indexing from zero ,decrease by 1 \r\n",
        "    score=1 #increase its importance to 100%\r\n",
        "  newscorelist.append(score)\r\n",
        "  pos=pos+1 #to get the next sentence from frame\r\n",
        "\r\n",
        "\r\n",
        "       \r\n",
        "\r\n",
        "analyzedfile_obj.write(\"\\n\\n The individual scores of all sentences are :- \\n {0}\".format(score_list,2))\r\n",
        "analyzedfile_obj.write(\"\\n\\n The normalized scores  (based on average score ) are {0} \\n \".format(newscorelist,2))\r\n",
        "#df['sentence_wordcount']=word_count_list\r\n",
        "df['sent_score']=score_list         #add the new collumn of individual sentence normalized score based onits length\r\n",
        "df['normalized_score']=newscorelist # store score based on average score of all sentences in text\r\n",
        "df.to_csv(features_filename) \r\n",
        "\r\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-22:13:16:16,506 INFO     [iwn.py:43] Loading gujarati language synsets...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original word was  ભુ\n",
            "updated word after  normalizeing ભૂ\n",
            "original word was  કૂ\n",
            "updated word after  normalizeing કુ\n",
            "original word was  કૂ\n",
            "updated word after  normalizeing કુ\n",
            "original word was  ભુ\n",
            "updated word after  normalizeing ભૂ\n",
            "tokens / words extracted are  ['એમણ', 'ચોરી', 'જ', 'જાતે', 'તરવાડ', 'દલ', 'ભૂએ', 'લઇ', 'વશરામ', 'વાડી', 'શાકભાજી']\n",
            "Original freq values of words considered were  {'એમણ': 7.478856748649475, 'ચોરી': 3.4445697543079654, 'જ': 4.645022527795701, 'જાતે': 3.225862516560162, 'તરવાડ': 4.694642245260513, 'દલ': 5.987692384057975, 'ભૂએ': 3.9148916311957622, 'લઇ': 3.2809930189218455, 'વશરામ': 6.538579205476664, 'વાડી': 6.789246734068492, 'શાકભાજી': 4.01852838885774}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NhW25DXfWop",
        "outputId": "a25143d0-68f8-4a8f-e958-4becc585322f"
      },
      "source": [
        "\r\n",
        "rougescorefile=\"Rougeresults-all-TF.csv\"\r\n",
        "rougescorefilehandle=open(rougescorefile,mode='a',encoding=\"utf-8\") #to store only the rouge scores for later analysis \r\n",
        "\r\n",
        "resultvar = csv.writer(rougescorefilehandle, lineterminator='\\n')\t\t#to store new row for each f  \r\n",
        "\r\n",
        "##########################################################\r\n",
        "################# ######Step4#############################\r\n",
        "################ Find top sentences  #################\r\n",
        "##########################################################\r\n",
        "#rank and sort the sentences  based on individual sentences score \r\n",
        "\r\n",
        "optimalratio=(totalnormgoldsentences/totalnormsentences)*100 #ideal ratio as per gold text given \r\n",
        "optimalratio = float(f\"{optimalratio :.2f}\")\r\n",
        "data=df.sort_values([\"sent_score\"],ascending=False,kind='quicksort')\r\n",
        "resultlist=[]#to store the rouge score data\r\n",
        "for percent in [optimalratio]:\r\n",
        "  resultlist.append(\"Extraction ratio\"+str(percent)+\",\")\r\n",
        "  analyzedfile_obj.write(\"\\n\\nThe method of summary is *** {0}% *** of original text\".format(percent))\r\n",
        "  n= round(totalnormsentences*int(percent)/100)\r\n",
        "  top_sent=data.iloc[0:n,1:2]  \r\n",
        " # print(top_sent.head())\r\n",
        "  toplist=list(top_sent.sentence)\r\n",
        " \r\n",
        "  analyzedfile_obj.write(\"\\n\\n List of top sentences are as follows-{0} \".format(toplist))\r\n",
        "  ##########################################################\r\n",
        "################# ######Step6#############################\r\n",
        "################ Generate summary  #################\r\n",
        "##########################################################\r\n",
        "  summary=[]\r\n",
        "  not_in_summary=[]\r\n",
        "  sentence_number=0\r\n",
        "  for sent in corpora:  #take one by one the sentences considered in feature vector \r\n",
        "    if sent in toplist: #if sentence is in topranked sentences then store its number so its original line can be fetched from original corpus\r\n",
        "        summary.append(sentence_number)\r\n",
        "    else:\r\n",
        "        not_in_summary.append(sentence_number)\r\n",
        "    sentence_number=sentence_number+1\r\n",
        "\r\n",
        "  analyzedfile_obj.write(\"\\n\\n ***** The summary geneated has the following numbered sentences ---->>\\n {0}\".format(summary))\r\n",
        "  analyzedfile_obj.write(\"\\n\\n ***** The sentences not included in summary are these numbers---->>{0}\".format(not_in_summary))\r\n",
        "  \r\n",
        "  counter=0 #now fetching the original lines to be copied in final summary. \r\n",
        "  summary_lines=[]\r\n",
        "  for sent in normcorpora:\r\n",
        "    if counter in summary:\r\n",
        "      summary_lines.append(sent)   \r\n",
        "    counter=counter+1\r\n",
        "\r\n",
        " \r\n",
        "  #store the summary in file\r\n",
        "  summary_filename='system-temp-{0}.txt'.format(str(percent))\r\n",
        "    #to give name tosummary file for storing percent based results\r\n",
        "  #format is  original file name -system-<percent> Ex:- news1-system-50\r\n",
        "  \r\n",
        "  with io.open(summary_filename,'w',encoding='utf-8') as w: \r\n",
        "    for i in summary_lines:\r\n",
        "      w.write(i)\r\n",
        "  \r\n",
        "  ###########step5#############\r\n",
        "  ########### Post Processing ######\r\n",
        "  resultfilename=\"system_summary_TF+stopwordr+stem_\"+filename\r\n",
        "  \r\n",
        "  post_process(summary_filename,resultfilename)#do the post processing and write in result file\r\n",
        "################# ######Step6#############################\r\n",
        "################ Evaluate summary  #################\r\n",
        "  guj_remove_newlines(norm_gold_file,norm_gold_file)\r\n",
        "  guj_remove_newlines(summary_filename,summary_filename)\r\n",
        "  rougescores=evalate_files(norm_gold_file,summary_filename)#get the evaluation of generated summary after post processing based on rouge score\r\n",
        "  for key in rougescores.keys():\r\n",
        "     resultlist.append(\"%s,%s\"%(key,rougescores[key]))\r\n",
        "  resultvar.writerow(resultlist)   \r\n",
        "  analyzedfile_obj.write(\"\\n **********************Rouge scores based analysis for {0} % extraction is as follows*********************** \".format(percent))\r\n",
        "  analyzedfile_obj.write(\"\\n\\n The rouge scores on summary file are \\n {0}\".format(rougescores,2)) #store the results \r\n",
        "\r\n",
        "  basicfile=\"original.txt\"\r\n",
        "  #guj_clean_text(filename,basicfile) # Get the original file in cleaned form for computing compression ratio. \r\n",
        "  guj_remove_newlines(filename,basicfile) \r\n",
        "  \r\n",
        "  sent_compression,word_compression,totaloriginalsentences,totaloriginalwords,totalsumsentences,totalsumwords=compression_statistics(summary_filename,basicfile) #to store the compression ratio analysis\r\n",
        "  analyzedfile_obj.write(\"\\n **********************Compression Analysis at {0} % extraction is as follows *********************** \".format(percent))\r\n",
        "  analyzedfile_obj.write(\"\\n Total sentences in original text were {0} \".format(totaloriginalsentences)) \r\n",
        "  analyzedfile_obj.write(\"\\n Total sentences in summary text are {0} \".format(totalsumsentences))\r\n",
        "  analyzedfile_obj.write(\"\\n Sentences compression ratio is {0} \\n\".format(sent_compression,2))\r\n",
        "  analyzedfile_obj.write(\" \\n\\n Total words in original text were {0} \".format(totaloriginalwords))\r\n",
        "  analyzedfile_obj.write(\" \\n total words in summary text are {0} \".format(totalsumwords))\r\n",
        "  analyzedfile_obj.write(\"\\n Words compression ratio is {0} \".format(word_compression,2)) \r\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.8066914448147483, 'p': 0.7977941176470589, 'r': 0.8157894736842105}, 'rouge-2': {'f': 0.7350746218662982, 'p': 0.7269372693726938, 'r': 0.7433962264150943}, 'rouge-l': {'f': 0.8289473634288436, 'p': 0.7974683544303798, 'r': 0.863013698630137}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wix3MhE06QRs"
      },
      "source": [
        "analyzedfile_obj.close()\n",
        "\n",
        "rougescorefilehandle.close()"
      ],
      "execution_count": 153,
      "outputs": []
    }
  ]
}