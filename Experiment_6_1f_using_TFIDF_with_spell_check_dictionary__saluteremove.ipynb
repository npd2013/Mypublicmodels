{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment 6.1f- using TFIDF with spell_check dictionary _saluteremove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1P9EYNZHT17tVxh0rqG75yxNplhJzW8yN",
      "authorship_tag": "ABX9TyP/gqVtQghF5fJkWZk1UZ2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npd2013/Mypublicmodels/blob/main/Experiment_6_1f_using_TFIDF_with_spell_check_dictionary__saluteremove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2jwlnJOe0wa"
      },
      "source": [
        "SETUP1:-  \n",
        "-Single document text summarization \n",
        "\n",
        "-frequency based method for scoring sentences \n",
        "\n",
        "-preprocessing applied are knowledge based normalization,  stopword removal and stemming.\n",
        "-NO CUE WORDS \n",
        "\n",
        "-automatic evaluation using rouge and compression ratio analysis.\n",
        "\n",
        "note:results stored in another file \n",
        "\n",
        "\n",
        "Algorithm steps :- \n",
        "\n",
        "1) Preprocessing of text- \n",
        "\n",
        "    1.1)Cleaning \n",
        "\n",
        "    1.1.1)Removal of unnecessary symbols/charachters like html tags ,extra spaces,extra newlines etc.\n",
        "         \n",
        " \n",
        " 1.2) Line segmentation\n",
        " \n",
        "    -using indicnlp with own segmenter  \n",
        "    -Divide text into lines and find token words using own word segmenter.\n",
        " 1.3) word segmentation\n",
        "    -Divide the lines into words.\n",
        "    -words can be separated by whitespace or characters like quotes or comma.  use the punctuation list to exclude from the word.Only extract valid words \n",
        "\n",
        "  1.4) Stopword removal \n",
        " \n",
        "       -basic set of 15 words removed from lines. \n",
        "   \n",
        " 1.5) Stemming\n",
        "    -if word is multiword expression,return  \n",
        "\n",
        "    -make dictionary of valid words using indowordnet \n",
        " \n",
        "    -remove salutations from suffix like bhai,bahen etc\n",
        " \n",
        "    -Correct wordspelling as per dictionary, if valid dictionary word. \n",
        "\n",
        "    -if not valid dictionary word\n",
        "\n",
        "       -look up prefix and trim if match found\n",
        "\n",
        "       -look up replacement and trim if match found\n",
        "\n",
        "       - look up longest suffix rules matching and trim if match found.\n",
        "    \n",
        "    -if trimmed word size>=3 \n",
        "            \n",
        "            -    stemword = trimword\n",
        "   \n",
        "                   \n",
        "               \n",
        "       \n",
        "\n",
        "2)Make vector space representation -based on term frequency\n",
        "\n",
        "    2.1) Compute tfidf using sklearn module of python.\n",
        "\n",
        "    2.2) For each sentence - \n",
        "\n",
        "     2.2.1) Compute sentence score= total tfidf score of all words in sentence divide by number of words in sentence.\n",
        "     2.2.2) If it is first or last sentence , increase its weightage ten times.\n",
        "  \n",
        "  2.3) For each sentence -\n",
        "\n",
        "     3.2.1) Compute the average normalized score  : \n",
        "          Current sentence score divide by total sentence score of text.\n",
        "      \n",
        "\n",
        " 4) //Find top \"n\" scored sentences.\n",
        "\n",
        " 4.1) Set n= number of lines in gold summary. 0r \"x\" percent of original text.    \n",
        " 4.2) Summary_length=n\n",
        " 4.3) Sorted_sentences=sort(sentences) based on their scores.\n",
        "    \n",
        " 5) //Generate the summary -\n",
        "\n",
        "    for sentences = 1 to gold_summary_length \n",
        "      - if sentence in gold_summary  \n",
        "           put sentence in system_summary. \n",
        "  \n",
        "6)// Evaluate the summary-\n",
        "\n",
        "    6.1) use the Rouge score \n",
        "      - Get the precision ,recall and F-measure for rouge-1 ,rouge2 and rouge-LCS.   \n",
        "\n",
        "    6.2) Find comression ratio of text\n",
        "       6.2.1) sentence compression ratio\n",
        "        = total sentences in system_summary/total sentences in source text \n",
        "        6.2.2) Word compression ratio\n",
        "        =total words in system_summary/total words in source \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeLGOonUpR4z",
        "outputId": "72b458aa-d1b1-406e-b702-efb4c17285e4"
      },
      "source": [
        "#install needed packages \n",
        "#!pip install  gensim  #has word2vec model built, using english \n",
        "\n",
        "!pip install pyiwn\n",
        "!pip install -U scikit-learn\n",
        "!pip install rouge\n",
        "!pip install indic-nlp-library\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyiwn\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/80/55d7412ed602071de200b74d75f7cba8c9b4fe6a2f3df33c6d5cd0e6cb83/pyiwn-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyiwn) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyiwn) (1.1.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (2020.12.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pyiwn) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pyiwn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyiwn) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pyiwn) (1.15.0)\n",
            "Installing collected packages: pyiwn\n",
            "Successfully installed pyiwn-0.0.5\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting indic-nlp-library\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/51/f4e4542a226055b73a621ad442c16ae2c913d6b497283c99cae7a9661e6c/indic_nlp_library-0.71-py3-none-any.whl\n",
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Installing collected packages: morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.71 morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7gUXWZMwb6D"
      },
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/guj_final_preporcess-v6.py\" \"guj_final_preporcess.py\" #The preprocessing modules designed\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/Evaluation.py\" \"Evaluation.py\"   # the evaluation module designed\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/guj_final_postprocessing-v1.py\" \"post.py\"   # Get the post processing module \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo4oVUa5g9D-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4664e480-bc90-4aeb-e0d4-dabefd8d0413"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import io\n",
        "import csv\n",
        "from collections import Counter\n",
        "from Evaluation import *\n",
        "from post import *\n",
        "import pandas as pd \n",
        "from guj_final_preporcess import * #to get the required preporcessing modules -from self made pipeline\n",
        "import pyiwn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-24:02:36:49,391 INFO     [helpers.py:20] Downloading IndoWordNet data of size ~31 MB...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-24:02:36:51,3 INFO     [helpers.py:43] Extracting /root/iwn_data.tar.gz into /root...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-24:02:36:52,231 INFO     [helpers.py:48] Removing temporary zip file from /root/iwn_data.tar.gz\n",
            "2021-02-24:02:36:52,240 INFO     [helpers.py:51] IndoWordNet data successfully downloaded at /root/iwn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XlBVjZMvCPA",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "2ab1348b-9ebc-40e0-b547-0d2990fe4c69"
      },
      "source": [
        "from google.colab import files #upload the files from local machine on the colab environment \n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-962b8b2a-e94f-41e3-b3cb-37a4c69a21b0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-962b8b2a-e94f-41e3-b3cb-37a4c69a21b0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving story1.txt to story1.txt\n",
            "Saving story1-gold.txt to story1-gold.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu3f_IRUd2NW",
        "outputId": "68c3fc1e-8d3f-4fb1-85ad-7d73a075f909"
      },
      "source": [
        "\r\n",
        "#take all the needed inputs from user \r\n",
        "basename=input('Enter the basic name of file(without txt),other names wud be autogenerated  ')\r\n",
        "filename=basename+'.txt'\r\n",
        "gold_filename=basename+'-gold.txt'\r\n",
        "#filename=input('Enter text file name to be summarized ') #Ex: file1.txt\r\n",
        "analysefile_name=filename.replace('.txt','analysis-TFIDF .txt')# to store the data for later analysis . Example- file1-results.txt \r\n",
        "features_filename=filename.replace('.txt','TFIDF-Features.csv') # file-1-features.csv\r\n",
        "#gold_filename=input(\"Enter name of file containing gold summary  \") #example file1-gold.txt ,assumes the name is having pattern ending with gold.txt \r\n",
        "summary_filename=filename.replace('gold.txt','-TFIDF+ALL_systemsummary.txt') # example file1-gold-system-summary.txt\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the basic name of file(without txt),other names wud be autogenerated  story1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZfHoaVCaViy"
      },
      "source": [
        "#read the input original file . \r\n",
        "\r\n",
        "\r\n",
        "guj_clean_html(filename,filename)# text cleaning remove the html tags and overwrite the original file \r\n",
        "guj_clean_html(gold_filename,gold_filename)# clean the html tags from gold file \r\n",
        "\r\n",
        "#Display the given data in input files \r\n",
        "with io.open(filename,'r',encoding='utf-8') as f: \r\n",
        "\t\t  lines=f.read()#get all the lines from file\r\n",
        "analyzedfile_obj=open(analysefile_name,mode='w',encoding='UTF-8')\r\n",
        "analyzedfile_obj.write(\"\\n\\n The original file is :------\\n\")\r\n",
        "analyzedfile_obj.write(lines)\r\n",
        "\r\n",
        "with io.open(gold_filename,'r',encoding='utf-8') as f: \r\n",
        "\t\t  lines=f.read()#get all the lines from file\r\n",
        "\r\n",
        "analyzedfile_obj.write(\"\\n\\n The gold summary provided by expert  is -->\\n\")    \r\n",
        "analyzedfile_obj.write(lines)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# now find the statistics of originally given informations \r\n",
        "originalcorpora,totaloriginalsentences=guj_corpus_generate(filename) #make corpus of original data \r\n",
        "goldcorpora,totalgoldcorpora=guj_corpus_generate(gold_filename) #make corpus of gold \r\n",
        "\r\n",
        "analyzedfile_obj.write(\"\\n\\n STATISTICS------>\")\r\n",
        "analyzedfile_obj.writelines(\"\\n Original file sentences(RAW) - {0}\\n\".format(totaloriginalsentences))\r\n",
        "analyzedfile_obj.writelines(\"\\n gold file sentences(RAW)- {0}\\n\".format(totalgoldcorpora))\r\n",
        "\r\n",
        "###################################################################\r\n",
        "########################  Step-1 ##################################\r\n",
        "#####################  cleaning phase  ########################\r\n",
        "\r\n",
        "#doing cleaning of file \r\n",
        "#remove newlines and other unnecessary data and clean it for feature extraction \r\n",
        "#to replace the data as per our format, replacing abbreviations dot with '*'\r\n",
        "normfile=\"temp1.txt\"\r\n",
        "norm_gold_file=\"temp2.txt\"\r\n",
        "\r\n",
        "guj_clean_text(filename,normfile) \r\n",
        "guj_clean_text(gold_filename,norm_gold_file)\r\n",
        "\r\n",
        "\r\n",
        "#Now display the stats and make corpora after cleaning file and removing new lines . \r\n",
        "normcorpora,totalnormsentences=guj_corpus_generate(normfile)  \r\n",
        "goldcorpora,totalnormgoldsentences=guj_corpus_generate(norm_gold_file) \r\n",
        "\r\n",
        "\r\n",
        "analyzedfile_obj.write(\" Segmented Lines in source -{0} \\n \".format(totalnormsentences))\r\n",
        "analyzedfile_obj.write(\" Segmented Lines in gold -{0}\\n \".format(totalnormgoldsentences))\r\n",
        "\r\n",
        "###################################################################\r\n",
        "########################  Step-2 ##################################\r\n",
        "#####################  Normalization and tokenization  for feature extraction  ########################\r\n",
        "\r\n",
        "cleanfile=\"file.txt\"\r\n",
        "mydictionary=guj_makedictionary()\r\n",
        "guj_stopwordremoval(normfile,cleanfile) #remove stopwords\r\n",
        "guj_stemmer_new(cleanfile,cleanfile,mydictionary)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#####\r\n",
        "\r\n",
        "\r\n",
        "#Now working on the modified file to get various features \r\n",
        "\r\n",
        "corpora,totalsent=guj_corpus_generate(cleanfile) #divide lines into sentences-\r\n",
        "analyzedfile_obj.write(\" Lines in source after all processing -{0} \\n \".format(totalsent))\r\n",
        "\r\n",
        "sent_pos_list=[] #to store the sentence position\r\n",
        "sent_words_count=[]  # to store the total words in each sentence \r\n",
        "sent_number=0\r\n",
        "for s in corpora:\r\n",
        "  sent_pos_list.append(sent_number)\r\n",
        "  sent_number+=1\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "# to generate vector based on frequency \r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "vectorizer = TfidfVectorizer(tokenizer = guj_mytokenizer,min_df=0.15,max_df=0.85) #frequency of words with atleast 15% frequency\r\n",
        "#vectorizer = TfidfVectorizer(tokenizer = guj_mytokenizer)\r\n",
        "cwm = vectorizer.fit_transform(corpora) #vectorize the corpus of input file\r\n",
        "\r\n",
        "\r\n",
        "tokens = vectorizer.get_feature_names() #get the names of words used in vector\r\n",
        "\r\n",
        "score_list=[]\r\n",
        "counts=cwm.sum(axis=1) #sums the sentence wise words' tfidf scores\r\n",
        "scorelist=list(counts.flat)#get the list of sentencescores \r\n",
        "\r\n",
        "\r\n",
        "analyzedfile_obj.write(\"\\n\\n****The TFIDF  based vector is formed ****\")\r\n",
        "analyzedfile_obj.write('\\n The dimension of vector matrix generated is -\\n {0} '.format(cwm.shape))\r\n",
        "\r\n",
        " \r\n",
        " \r\n",
        "list_of_tuples = list(zip(sent_pos_list,corpora, cwm.toarray()[0]))  \r\n",
        "#generate a dataframe to store the data \r\n",
        "import pandas as pd \r\n",
        "df=pd.DataFrame(list_of_tuples, columns=['sent_pos','sentence','frequency_vector'])\r\n",
        "df.to_csv(features_filename) \r\n",
        "\r\n",
        "\r\n",
        "##########################################################\r\n",
        "################# ######Step3#############################\r\n",
        "################ Compute sentence scores #################\r\n",
        "##########################################################\r\n",
        "\r\n",
        "score_list=[] #to store sentencewise scores original \r\n",
        "totwords=[]\r\n",
        "for s in corpora:\r\n",
        "   words=guj_mytokenizer(s)\r\n",
        "   #print(\"number of words in sentence are\",len(words))\r\n",
        "   totwords.append(len(words))\r\n",
        "newscorelist=[]\r\n",
        "\r\n",
        "list_of_tuples = list(zip(sent_pos_list,corpora, cwm.toarray(),scorelist,totwords))  \r\n",
        "df=pd.DataFrame(list_of_tuples, columns=['sent_pos','sentence','feature_vector','totalsentcount','totalwordsinsent'])\r\n",
        "\r\n",
        "df['sent_score']=df['totalsentcount']/df['totalwordsinsent']\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df.to_csv(features_filename) \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NhW25DXfWop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ee3a14-3678-4bb5-ea3e-ae4492623d49"
      },
      "source": [
        "\r\n",
        "rougescorefile=\"Rougeresults-all-TF.csv\"\r\n",
        "rougescorefilehandle=open(rougescorefile,mode='a',encoding=\"utf-8\") #to store only the rouge scores for later analysis \r\n",
        "\r\n",
        "resultvar = csv.writer(rougescorefilehandle, lineterminator='\\n')\t\t#to store new row for each f  \r\n",
        "\r\n",
        "##########################################################\r\n",
        "################# ######Step4#############################\r\n",
        "################ Find top sentences  #################\r\n",
        "##########################################################\r\n",
        "#rank and sort the sentences  based on individual sentences score \r\n",
        "\r\n",
        "optimalratio=(totalnormgoldsentences/totalnormsentences)*100 #ideal ratio as per gold text given \r\n",
        "optimalratio = float(f\"{optimalratio :.2f}\")\r\n",
        "data=df.sort_values([\"sent_score\"],ascending=False,kind='quicksort')\r\n",
        "resultlist=[]#to store the rouge score data\r\n",
        "for percent in [optimalratio]:\r\n",
        "  resultlist.append(\"Extraction ratio\"+str(percent)+\",\")\r\n",
        "  analyzedfile_obj.write(\"\\n\\nThe method of summary is *** {0}% *** of original text\".format(percent))\r\n",
        "  n= round(totalnormsentences*int(percent)/100)\r\n",
        "  top_sent=data.iloc[0:n,1:2]  \r\n",
        " # print(top_sent.head())\r\n",
        "  toplist=list(top_sent.sentence)\r\n",
        " \r\n",
        "  analyzedfile_obj.write(\"\\n\\n List of top sentences are as follows-{0} \".format(toplist))\r\n",
        "  ##########################################################\r\n",
        "################# ######Step6#############################\r\n",
        "################ Generate summary  #################\r\n",
        "##########################################################\r\n",
        "  summary=[]\r\n",
        "  not_in_summary=[]\r\n",
        "  sentence_number=0\r\n",
        "  for sent in corpora:  #take one by one the sentences considered in feature vector \r\n",
        "    if sent in toplist: #if sentence is in topranked sentences then store its number so its original line can be fetched from original corpus\r\n",
        "        summary.append(sentence_number)\r\n",
        "    else:\r\n",
        "        not_in_summary.append(sentence_number)\r\n",
        "    sentence_number=sentence_number+1\r\n",
        "\r\n",
        "  analyzedfile_obj.write(\"\\n\\n ***** The summary geneated has the following numbered sentences ---->>\\n {0}\".format(summary))\r\n",
        "  analyzedfile_obj.write(\"\\n\\n ***** The sentences not included in summary are these numbers---->>{0}\".format(not_in_summary))\r\n",
        "  \r\n",
        "  counter=0 #now fetching the original lines to be copied in final summary. \r\n",
        "  summary_lines=[]\r\n",
        "  for sent in normcorpora:\r\n",
        "    if counter in summary:\r\n",
        "      summary_lines.append(sent)   \r\n",
        "    counter=counter+1\r\n",
        "\r\n",
        " \r\n",
        "  #store the summary in file\r\n",
        "  summary_filename='system-temp-{0}.txt'.format(str(percent))\r\n",
        "    #to give name tosummary file for storing percent based results\r\n",
        "  #format is  original file name -system-<percent> Ex:- news1-system-50\r\n",
        "  \r\n",
        "  with io.open(summary_filename,'w',encoding='utf-8') as w: \r\n",
        "    for i in summary_lines:\r\n",
        "      w.write(i)\r\n",
        "  \r\n",
        "  ###########step5#############\r\n",
        "  ########### Post Processing ######\r\n",
        "  resultfilename=\"system_summary_TF+stopwordr+stem_\"+filename\r\n",
        "  \r\n",
        "  post_process(summary_filename,resultfilename)#do the post processing and write in result file\r\n",
        "################# ######Step6#############################\r\n",
        "################ Evaluate summary  #################\r\n",
        "  guj_remove_newlines(norm_gold_file,norm_gold_file)\r\n",
        "  guj_remove_newlines(summary_filename,summary_filename)\r\n",
        "  rougescores=evalate_files(norm_gold_file,summary_filename)#get the evaluation of generated summary after post processing based on rouge score\r\n",
        "  for key in rougescores.keys():\r\n",
        "     resultlist.append(\"%s,%s\"%(key,rougescores[key]))\r\n",
        "  resultvar.writerow(resultlist)   \r\n",
        "  analyzedfile_obj.write(\"\\n **********************Rouge scores based analysis for {0} % extraction is as follows*********************** \".format(percent))\r\n",
        "  analyzedfile_obj.write(\"\\n\\n The rouge scores on summary file are \\n {0}\".format(rougescores,2)) #store the results \r\n",
        "\r\n",
        "  basicfile=\"original.txt\"\r\n",
        "  #guj_clean_text(filename,basicfile) # Get the original file in cleaned form for computing compression ratio. \r\n",
        "  guj_remove_newlines(filename,basicfile) \r\n",
        "  \r\n",
        "  sent_compression,word_compression,totaloriginalsentences,totaloriginalwords,totalsumsentences,totalsumwords=compression_statistics(summary_filename,basicfile) #to store the compression ratio analysis\r\n",
        "  analyzedfile_obj.write(\"\\n **********************Compression Analysis at {0} % extraction is as follows *********************** \".format(percent))\r\n",
        "  analyzedfile_obj.write(\"\\n Total sentences in original text were {0} \".format(totaloriginalsentences)) \r\n",
        "  analyzedfile_obj.write(\"\\n Total sentences in summary text are {0} \".format(totalsumsentences))\r\n",
        "  analyzedfile_obj.write(\"\\n Sentences compression ratio is {0} \\n\".format(sent_compression,2))\r\n",
        "  analyzedfile_obj.write(\" \\n\\n Total words in original text were {0} \".format(totaloriginalwords))\r\n",
        "  analyzedfile_obj.write(\" \\n total words in summary text are {0} \".format(totalsumwords))\r\n",
        "  analyzedfile_obj.write(\"\\n Words compression ratio is {0} \".format(word_compression,2)) \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.5911949635851915, 'p': 0.642369020501139, 'r': 0.5475728155339806}, 'rouge-2': {'f': 0.46008402864531106, 'p': 0.5, 'r': 0.4260700389105058}, 'rouge-l': {'f': 0.5807560087742825, 'p': 0.6282527881040892, 'r': 0.5399361022364217}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wix3MhE06QRs"
      },
      "source": [
        "analyzedfile_obj.close()\n",
        "\n",
        "rougescorefilehandle.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}