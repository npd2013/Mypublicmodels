# -*- coding: utf-8 -*-
"""guj_preprocess_modules.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gVCtIAx0l3hMwN1oec77kdT8B0Hf9ErJ
@author: NIKITA

"""
##!cp "/content/drive/My Drive/Colab Notebooks/guj_final_preporcess-v6.py" "guj_final_preporcess.py" 
# command to include this #file in the RE of colab 
def guj_web_scrap_multiplepage(seed,filename):
 import io

 

 import wikipedia as wk
 wk.set_lang('gu')
 firstpage=wk.page(1)
 try:
    #firstpage=wk.page("મુનશી પ્રેમચદ ")
    firstpage=wk.page(seed) #start scrapping wiki pages from this seed word
    data=firstpage.content

   # print('data is',data)
    with io.open(filename,'a+',encoding='utf8') as f:
                f.write(data) 
 except wk.exceptions.PageError:
    pass

 if firstpage:
    extralinks=firstpage.links
   

    for i in extralinks:
        #if count > 0:
    
                try:
                    newpage=wk.page(i)
                    data=newpage.content
                   # count=count-1
                    with io.open(filename,'a+',encoding='utf8') as f:#append to file if it exists otherwise create new one
                        f.write(data) 
            
                except wk.exceptions.PageError:
                    pass



 return(filename)
              
                



"""WEB SCRAPPING ::: FETCHING DATA FOR WORKING (FROM WIKIPEDIA)(small file)"""
def guj_web_scrap_singlepage(seed,filename):
 import io
 import wikipedia as wk

 wk.set_lang('gu')
 firstpage=wk.page(1)
 try:
	    firstpage=wk.page(seed) #get information about the given seed word in wiki
	    data=firstpage.content
   #print('data is',data)
	    with io.open(filename,'a+',encoding='utf8') as f:
        	        f.write(data) 
 except wk.exceptions.PageError:
      return(filename)
  #print("the name returned back is ",datafile)    
 return(filename)
 

 
def guj_clean_html(originalfile,resultfile):
    #remove the html tags and update the given input file
  import re
  import io  
  
  with io.open(originalfile,'r',encoding='utf-8') as f: 
       lines=f.read()
  lines=re.sub('\<DOCNO\>.*\</DOCNO\>','',lines)# remove the document tags
  lines=re.sub('\<.*\>',' ',lines)#remove all other tags
  lines=re.sub(r'\ufeff','',lines) #replace the special encoding code mentioned at start of html files
  
  with io.open(resultfile,'w',encoding='utf-8') as w:
   w.write(lines) 
               
  return(resultfile)
  
  




def guj_clean_text(originalfile,resultfile):
    #replace the inconsistent usage of quotes after sentence end.
 from indicnlp.tokenize import sentence_tokenize
 import re
 import io        
 
 with io.open(originalfile,'r',encoding='utf-8') as f: 
       lines=f.read()
 lines=re.sub(r"\n+"," ",lines) #remove  extra newlines 
 lines=re.sub(r"!+","!",lines) #remove multiple exclamations with single
 lines=re.sub(r"[ ]+"," ",lines) #removes extra whitespaces 
 lines=re.sub(r"\?+","?",lines) #remove multiple question mark with single
 lines=re.sub(r"\.+",".",lines) #replace multiple dot marks with single
 lines=re.sub(r"\.\s*’","’. ",lines)#.Pattern :- start with dot(escape the sp meaning)  followe by zero or more whitespace nd then quotes
 lines=re.sub(r"\.\s*'","'. ",lines)#single quotes handled
 lines=re.sub(r'\.\s*"','". ',lines)#double quotes handled
 lines=re.sub(r'\.\s*”','”. ',lines)#double quotes handled
 lines=re.sub(r"\.\s*’’","’’. ",lines)#single quotes written twice handled
 
 lines=re.sub(r"\?\s*’","’? ",lines)#single quotes handled
 lines=re.sub(r"\?\s*'","'? ",lines)#single quotes handled
 lines=re.sub(r'\?\s*"','"? ',lines)#double quotes handled
 lines=re.sub(r'\?\s*”','”? ',lines)#double quotes handled
 lines=re.sub(r"\?\s*’’","’’? ",lines)#single quotes written twice handled
 
 lines=re.sub(r"!\s*’","’! ",lines)#single quotes handled
 lines=re.sub(r"!\s*'","'! ",lines)#single quotes handled
 lines=re.sub(r'!\s*"','"! ',lines)#double quotes handled
 lines=re.sub(r'!\s*”','”! ',lines)#double quotes handled
 lines=re.sub(r"\!\s*’’","’’! ",lines)#single quotes written twice handled
 #list of generally found valid sentence boundary words which can be followd by dot .
 #For others,heuristic of lenght is used.(if length of word having dot as suffix is less than 3 , it is possibly an abbreviation )
 boundarylist=['હતાં','હતા','હતું','હતી','હતો','હતો','ગઈ','ગયો','ગઇ','છું','દે','નહિ','નથી','કરે','થયા','થઈ','થઇ','જશે','થતો','થશે','રહી','છે','છો','છુ','શકે','તે','જા','અહીં','થી','આવ','દો','કર','થાય','પર','જાય','માટે' ,'પરથી','આવ્યું','સુધી','થાય','હતો','થઈ','સાથે','લાગે','હોવા','છતાં','રહેલા','કર્યુ','જુઓ','જાઓ','શકો','નથી','થતો','નહી','હોત','હશે','હોવું','તો','લે']  
 listlines=sentence_tokenize.sentence_split(lines, lang='gu')
 replacedict={}
 for line in listlines: #find all dots for abbreviations .
   matchfloat=re.search(r'[૧૨૩૪૫૬૭૮૯0o]+\.',line)#find all the float numbers ,the intermediate dot shud be replaced by *
   if matchfloat:
     endpos=matchfloat.end() #get the index in line where the dot has occured in float number
     actualpos=endpos-1
     line = line[: actualpos] + '*' + line[actualpos + 1:] # join the previous part with replaced part and then rest of the string
    # print("updated after float replacements ",line)
   matchingwords=re.findall('[\S]+[\.]',line)  #find all prefix with dot at end in one line 
   #print("words with abbreviations are ",matchingwords)
   for m in matchingwords: #for each word having dot at end in the prefix list
      wordlist=re.split('\.',m) # get the wordlist having only words,except the dot
      
      for w in wordlist:  #  take each word 
       endquotes=["'",'"',"’","”"," ",""] #if ending with quotes or have blanks then ignore it .
       if w.endswith(tuple(endquotes)):
           continue                  
       if w not in boundarylist : # check is it allowed boundary word then dont modify the subsequent dot
        if len(w)<3:# Check is length of word found is small then it is  abbreviation
           originalword=w+'.' #take original word
           newword=w+'*' #make the replacement word
           #print("new word is ",newword)
           #print("old word is ",originalword)
          
           replacedict[originalword]=newword   #maintain the words which are found with abbreviations and their replacement strings with *
 
 
 for key,value in replacedict.items():
      if key=='.': #as space followed by dot also occurs sometimes, take care to exclude it from replacment
        continue
      if key in lines:  #find the word is in lines ,then replace it with word followed by *
        lines=lines.replace(key,value)

 with io.open(resultfile,'w',encoding='utf-8') as w:
       w.write(lines)
    
 
 
               
 return(resultfile)

 
 
 
 with io.open(resultfile,'w+',encoding='utf-8') as w:
       w.write(lines) 
               
 return(resultfile)

 
def guj_stopwordremoval(originalfile,resultfile):
#need to send the file name containing data to be tokenized. 
 import io
 import re
 #static words in stopword list

 stoplist=['હતાં','એમ','છે','છો','છુ','હતા','હતું','હતી','હોય','હતો','તેમાં','અને','કે','તથા','તો','છું']
 with io.open(originalfile,'r',encoding='utf-8') as f: 
		  lines=f.read()#get all the lines from file
 sentences,totalsentences=guj_corpus_generate(originalfile)		  
 
 for word in stoplist:
     lines=re.sub(r"%s "%(word), ' ',lines)#for stopword occuring inside line ,so separated by space 
     lines=re.sub(r"%s\."%(word),'.',lines)#if stopword occurs at end of line ,it would have different sentence end marks following,
     lines=re.sub(r"%s\!"%(word),'!',lines)# stopwrod can be followed by some special marks , retaint them. 
     lines=re.sub(r"%s\)"%(word),')',lines)
     lines=re.sub(r"%s\,"%(word),',',lines)
     lines=re.sub(r"%s\?"%(word),'?',lines)
     lines=re.sub(r"%s\”"%(word),'”',lines)
     lines=re.sub(r"%s\’"%(word),'’',lines)
 with io.open(resultfile,'w+',encoding='utf-8') as w:   
      w.write(lines) 
 return(resultfile)    
 
def guj_makedictionary():
 #returns dictionary made using indowordnet 
 import pyiwn
 iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.GUJARATI)
 listwords=iwn.all_words()#all words in wordnet ,store in a list 
 return(listwords)
 
def guj_stemmer_new(originalfile,resultfile,dictionary):
#using the dictionary to stem
 import io
 import re
 with io.open(originalfile,'r',encoding='utf-8') as f: 
		  lines=f.read()#get all the lines from file

 stemmedresultfile='stemmingdata.csv' #to store the original word and stem word found for all words in given file . 
 vocablist=guj_makevocab(originalfile) # make vocabulary list of words for given file. 
 #to generate the modified file 
 stemmedwordslist=[]
 lemmawordslist=[]
 replacedict={}
 mycorpora=guj_sentence_segmenter(originalfile) #get the sentences in datafile 
 with io.open(stemmedresultfile,'a+',encoding='utf-8') as s: #store the stemmed word set ,  with original word
  for word in vocablist:
      isdict=0 #Flag to identify it is dictionary word or not 
      sword,isdict=guj_stem_withdict(word,dictionary) #else find stemword and pass the indowordnet also to be used 
          #print('original word , stemmed word and flag ----> ',word,sword,isdict )
      if isdict==1:
              #print("got the dictionary word so adding in dictionary list ",sword)
              lemmawordslist.append(sword) # the dictionary word was found so it was lemma form
      else:
              stemmedwordslist.append(sword) #maintain list of all stemmings done
              
      if sword!=word:
          replacedict[word]=sword #make entry in replacement dictionary ,replace original word with this stemword
            
 #print("stemmed words list is \n ",stemmedwordslist)
 #print("dictionary words lsit is \n",lemmawordslist)
 with io.open(resultfile,'w+',encoding='utf-8') as w: #store the stemmed sentences back in file
        #divide lines into sentences
   for line in mycorpora:
     for word in guj_mytokenizer(line):
        for key,val in replacedict.items():
          if key==word:
      #          print("original word and replacement word are ",word,val)
                line=re.sub(r'%s[ ,)\.]'%word,val+" ",line) #replace if the word is entire match and not substring 
     #print("after replacement line is now -->",line)                
     w.write(line)#write the modified line in result file 
   
 return(resultfile,stemmedwordslist,lemmawordslist)


 
def guj_spellcheck(word,dictionary): # check the word in dictionary ,make corrections if needed and return back correct word
    #returns back word and status , if valid dictinary word returned then status is 1 else 0
    import io
    import re
     
                 
    if word in dictionary:
        return (word,1) #valid word ,so return it back and "1" indicates it is valid dictionary word
    replacementcharacters={#frequently inconsistent replacements are directly replaced
    "્ઋ":"ૃ","ગ્ન":"જ્ઞ","ઋ":"ૠ","ઑ": "ઓ",':' : '', "ઇ":"ઈ" , "જિ":"જી","ઊ":"ઉ","િ":"ી","ી":"િ","ુ":"ૂ","ૂ":"ુ"}
    
    for key in replacementcharacters.keys():
                      
                      if word.find(key) !=-1:
                         nword=re.sub(key,replacementcharacters[key],word)#else do the replacment 
                         
                         if nword in dictionary:
                              #print("original word was ",word)
                              #print("updated word after  normalizeing "+nword)
                              return (nword,1) 
                              
             
    return(word,0)#no mathces found in dictionary,so return word back as it is.    
               
def guj_sentence_segmenter(originalfile):
 # input is original text file and returns back the list of sentences formed using punkt tokenizer 
 from indicnlp.tokenize import indic_tokenize 
 from indicnlp.tokenize import sentence_tokenize
 import io
 import re
 with io.open(originalfile,'r',encoding='utf-8') as f: #get the original file content
         lines=f.read()
 sentences=sentence_tokenize.sentence_split(lines, lang='gu') #divide lines into sentences
 
 #print("text received for segmentation is ---> ",sentences)
 return(sentences)



             
def guj_mytokenizer(line):
    #find word tokens in a given line 
 import re
 import io
 tokens=[]
 excludelist=["!",".","(",")","`",",","-","?",'"','”',";",":","’","'",'“','”','”',"‘","“","*",'\"']
 
 #print("line to be tokenized is ",line)
 for word in line.split():#split based on whitespaces the words from given line 
        #print("word got by splitting was ",word)
        next=0
        for symbol in excludelist: 
           if word==symbol: #word is punctuation ,then dont include it as token and check  next word 
              next=1
              continue
           pos=word.find(symbol)
           if pos>-1 and len(word)>1: #symbol found in word and it is not the puntuation symbol
              word=word.strip(symbol)
              continue
           
        if next==0:
              tokens.append(word) #current word to be added as token
              
 
 #print("final tokens formed were ",tokens)
 return (tokens)
             
def guj_makevocab(datafile):
  import io
  mycorpora=[]
  mycorpora=guj_sentence_segmenter(datafile)
  #divide lines into sentences
  words=[]
  for line in mycorpora:
     for word in guj_mytokenizer(line):
        if word not in ['!',".","(",")","`",",","-","?",'"','”',";",":","’","‘","“","*"," ",")"]: #dont take punctutions in the vocabulory 
          words.append(word)
  words=set(words) #Get the unique words in entire text 
  vocab={} #store the words found in a dictionary 
  key=0
  for word in words:
    vocab[word]=key
    key=key+1
  
  return(vocab)      
           
    
 
def remove_salutes(word):
     import re
     if  (word.find("સાહેબ")>0):#saluation is after some name of minimum 1 charachter then update the word
         word=re.sub("સાહેબ","",word)
         return (word)
     if  (word.find("બહેન")>0):
         word=re.sub("બહેન","",word)
         return (word)
     if (word.find("ભાઈ")>0):
         word=re.sub("ભાઈ","",word)
         return (word)
     if  (word.find('ભાઇ')>0):
         word=re.sub("ભાઇ","",word)
         return (word) 
     if (word.find('બેન')>0):
         word=re.sub("બેન","",word)
         return (word)  
     return(word) #else return same word without any modification     

     
            
def guj_stem_withdict(word,dictionary):      
 import io
 import re
 if(word.find('-')!=-1): #word is a multiword expression ,so dont stem it 
   return (word,0) 
 listwords=dictionary#all words in wordnet
 #print("\ngiven word was ",word)
 word=remove_salutes(word)#words with bhai or bahen or saheb attached are stripped 
 #print(" \nword after salutation removal is - ",word)
 dword,status=guj_spellcheck(word,dictionary)#check the word in dictionary,if found equivalent word ,return that as replacment word and also if found original word , send status as 1 
 if status==1:
     #print("word after spell check was fount to match in dictionary so going back with ",dword)
     return (dword,1) #found dictionary word ,return it 
      
 suffixes ={  1: ["ો", "ી" ," ે", "ા" , "ૂ", "ે" , "ૢ", "ૄ" ,"ૈ" , "ૌ"  ,"ુ","્","ઈ","એ","ઇ"],
   2: ["ઓએ","ીઓ" ,"ીએ","ોએ" ,"ાઓ","ાઈ","ાઇ","થી","ના","ની","ને","નો" ,"નુ","મા","મી","મુ","શે" ,"ીશ","શો","મત","ઓ","એ" ,"ું","ંુ","ાં","ંા","ંૂ", "ૂં",
"તો","તી","તા","યા","યુજે","ાય","યો","કે","જો","જી","ેલ","વા","વી"] ,
            3: ["ાઓએ","ઓના", "ઓનુ","ઓમા","ઓનો","ઓની","ઓને","ઓથી","ઓના","નું","નાં","માં","તાં","યું","યાં","તું","મું","વાં","ોને" ,"ોની" ,"ાના","ાની" ,"ભરી","ાને",
"ાનો" ,"ોનુ","માન","ીતા", "કાળ","ોથી" ,"દાર", "જનો","ાવે","ાવી","ાવર","વું","ેથી","પણુ","ેલી","ેલા","ેલોરી","નાર"],
            4: [ "ીઓની","ઓનું","ઓનાં","ઓમાં","ાઓની" ,"ોઓથી" , "ાઓનો", "ાઓને","ાઓનુ","ોનાં" ,"ોનું" ,"ાનાં", "ાનું", "વાળા","વાળી","પૂ઼ણઁ","વેરા","વાળો","પણું",
"માથી","ાવીશ",
"વાથી","વાના","વાનો","વાની","નારી","નારો","ીશુ","ેલાં","ેલું"] , 
            5: ["ાઓનું","માંથી" , "વાનાં","ીશું","પૂર્વક" ,"વામાં","વાનું" ,"વાળું","વવાની","વવાનો","યાનાં"],
   6:[ "વાળાએ","વાળાઓની", "વાળાઓનું","વાળાઓનુ", "વાળાની" , "વાળાનો","વાવાળા","ોમાંથી"  ]
   }

 prefixes =   ["બિન","ગેર","સર્વ", "અર્ધ"]
 replacementlist=    {"ન્યાં" :"ન","ન્યા" :"ન",
"ન્યાના": "ન","ન્યાની":"ન","ન્યાનો":"ન","ન્યા" :"ન",
"ન્યું":"ન", "ન્યુ":"ન", "ન્યો" :"ન", "વ્યાનાં" : "વ", "વ્યાના" : "વ","વ્યાની" : "વ", "વ્યો": "વ",
"વ્યાં" :"વ","વ્યા":"વ", "વ્યાના":"વ", "વ્યાનું":"વ", "વ્યાનુ":"વ",  "વ્યાનો" : "વ",
  "વ્યુ":"વ","વ્યે" :"વ", "પ્યા": "પ",
"પ્યું":"પ", "પ્યુ":"પ", "પ્યો":"પ" , "પ્યાં":"પ", "પ્યા":"પ","પ્યાની": "પ","પ્યાનો": "પ",
"પ્યુ" :"પ","ખ્યાનું":"ખ", "ખ્યાનુ":"ખ","ખ્યા":"ખ", "ખ્યું":"ખ","ખ્યુ":"ખ","ખ્યાં" :"ખ","ખ્યા" :"ખ",
"ખ્યાના":"ખ","ખ્યો":"ખ","ળ્યા":"ળ","ળ્યું" :"ળ","ળ્યુ" :"ળ","ળ્યાં" :"ળ","ળ્યા" :"ળ",
"ળ્યો": "ળ","લ્યાં" : "લ","લ્યા":"લ","લ્યું" :"લ","લ્યુ":"લ","લ્યો":"લ",
"લ્યે": "લ","વવુ":"વ","વવું":"વ","રું":"ર","વવો":"વ"

  }  # Extra similar replacements added to match the words without ending anusvar which are removed in normalizing
 
 while True:#infinite loop ,to continue for currentword only stop when no updation done 
  flag=0
  #print("in process for stemming word - ",word)
  #step1 Check if a number ,then remove the extra word inflection at end 
  number=["૧","૨","૩","૪","૫","૬","૭","૮","૯","0","૦"]
  if word.startswith(tuple(number)): #check is the given word a number? 
    regex = re.compile('[^૧૨૩૪૫૬૭૮૯0૦]')#if yes then remove its non numeric suffix part
    word=regex.sub('',word)
    return (word,0) #now return back, no other processing needed. 
  
  #step2 check if prefix matches ,then remove it 
  for pref in prefixes:
        if word.startswith(pref) and word>pref:  #check the word is only starting with this and is not entire word
            word=re.sub("^{0}".format(pref),"",word,1)
            flag=1
            break #continue with next step 
  #step3 Check the subsitution rules ,if found match ,replace  it .
            
  for key in replacementlist.keys():
        
        if word.endswith(key) and word>key: #check the word is actually end of string and not directly the string. 
          word=re.sub("{0}".format(key),"{0}".format(replacementlist[key]),word,1)
          flag=1
          return (word,0) #return back now no need to do more stemming ,else wud get overstemmed
         
      
   #step4 Check the suffix , if found match , remove it.          
  
  
  #start=len(word)#start checking from list with as per the extra symbols after 3 in source.
  #start=start-2
  #if start>6 : #as suffix of length 6 and less , so  we limit the start range from 6 
  #    start=6
  start=6 #as suffix are of length 6 and less , so  we start checking suffix from 7th place onwards in string 
  for L in  range(start,0,-1): #
            #print("word and L  and orginal word length is:: ",word,L,len(word))
            
            
         #   print("it entered suffix check of L--",L) 
            for suf in suffixes[L]:
               # print("the suffix considered is ",suf)
                if word.endswith(suf):
                   # print("the word was ending with this suffix*****",suf)
                    possibleword=re.sub("{0}$".format(suf),"",word)
                   # print("trimmed word and its length computed is ",possibleword,len(possibleword))
                    if len(possibleword)>=2: #if trimmed word becomes smaller than 2 size ,it might not be proper stem ,so dont use it 
                        word=possibleword #else use the trimmedword as the stemword
                        #print("new word formed after subsitution is ",word)
                        flag=1 #one replacement done ,maybe till end ..then try for other possible .
                        break
                    
            if flag==1:
              dword,status=guj_spellcheck(word,dictionary)#check the word in dictionary,if found equivalent word ,return that as replacment word
              if status==1:
                 return (dword,0) #found dictionary word after stemming ,so stop further trimming and return but keep flag off to indicate it was stemmed 
              
  
  if flag==0:
     break
 return (word,0) #return word and flag as not dictionary word 

def guj_stem_withoutdict(word):      
 import io
 import re
 suffixes ={  1: ["ો", "ી" ," ે", "ા" , "ૂ", "ે" , "ૢ", "ૄ" ,"ૈ" , "ૌ"  ,"ુ","્","ઈ","એ","ઇ"],
   2: ["ઓએ","ીઓ" ,"ીએ","ોએ" ,"ાઓ","ાઈ","ાઇ","થી","ના","ની","ને","નો" ,"નુ","મા","મી","મુ","શે" ,"ીશ","શો","મત","ઓ","એ" ,"ું","ંુ","ાં","ંા","ંૂ", "ૂં",
"તો","તી","તા","યા","યુજે","ાય","યો","કે","જો","જી","ેલ","વા","વી"] ,
            3: ["ાઓએ","ઓના", "ઓનુ","ઓમા","ઓનો","ઓની","ઓને","ઓથી","ઓના","નું","નાં","માં","તાં","યું","યાં","તું","મું","વાં","ોને" ,"ોની" ,"ાના","ાની" ,"ભરી","ાને",
"ાનો" ,"ોનુ","માન","ીતા", "કાળ","ોથી" ,"દાર", "જનો","ાવે","ાવી","ાવર","વું","ેથી","પણુ","ેલી","ેલા","ેલોરી","નાર"],
            4: [ "ીઓની","ઓનું","ઓનાં","ઓમાં","ાઓની" ,"ોઓથી" , "ાઓનો", "ાઓને","ાઓનુ","ોનાં" ,"ોનું" ,"ાનાં", "ાનું", "વાળા","વાળી","પૂ઼ણઁ","વેરા","વાળો","પણું",
"માથી","ાવીશ",
"વાથી","વાના","વાનો","વાની","નારી","નારો","ીશુ","ેલાં","ેલું"] , 
            5: ["ાઓનું","માંથી" , "વાનાં","ીશું","પૂર્વક" ,"વામાં","વાનું" ,"વાળું","વવાની","વવાનો","યાનાં"],
   6:[ "વાળાએ","વાળાઓની", "વાળાઓનું","વાળાઓનુ", "વાળાની" , "વાળાનો","વાવાળા","ોમાંથી"  ]
   }

 prefixes =   ["બિન","ગેર","સર્વ", "અર્ધ"]
 replacementlist=    {"ન્યાં" :"ન","ન્યા" :"ન",
"ન્યાના": "ન","ન્યાની":"ન","ન્યાનો":"ન","ન્યા" :"ન",
"ન્યું":"ન", "ન્યુ":"ન", "ન્યો" :"ન", "વ્યાનાં" : "વ", "વ્યાના" : "વ","વ્યાની" : "વ", "વ્યો": "વ",
"વ્યાં" :"વ","વ્યા":"વ", "વ્યાના":"વ", "વ્યાનું":"વ", "વ્યાનુ":"વ",  "વ્યાનો" : "વ",
  "વ્યુ":"વ","વ્યે" :"વ", "પ્યા": "પ",
"પ્યું":"પ", "પ્યુ":"પ", "પ્યો":"પ" , "પ્યાં":"પ", "પ્યા":"પ","પ્યાની": "પ","પ્યાનો": "પ",
"પ્યુ" :"પ","ખ્યાનું":"ખ", "ખ્યાનુ":"ખ","ખ્યા":"ખ", "ખ્યું":"ખ","ખ્યુ":"ખ","ખ્યાં" :"ખ","ખ્યા" :"ખ",
"ખ્યાના":"ખ","ખ્યો":"ખ","ળ્યા":"ળ","ળ્યું" :"ળ","ળ્યુ" :"ળ","ળ્યાં" :"ળ","ળ્યા" :"ળ",
"ળ્યો": "ળ","લ્યાં" : "લ","લ્યા":"લ","લ્યું" :"લ","લ્યુ":"લ","લ્યો":"લ",
"લ્યે": "લ","વવુ":"વ","વવું":"વ","રું":"ર","વવો":"વ"

  }  # Extra similar replacements added to match the words without ending anusvar which are removed in normalizing
 
 while True:#infinite loop ,to continue for currentword only stop when no updation done 
  flag=0
  #print("in process for stemming word - ",word)
  #step1 Check if a number ,then remove the extra word inflection at end 
  number=["૧","૨","૩","૪","૫","૬","૭","૮","૯","0","૦"]
  if word.startswith(tuple(number)): #check is the given word a number? 
    regex = re.compile('[^૧૨૩૪૫૬૭૮૯0૦]')#if yes then remove its non numeric suffix part
    word=regex.sub('',word)
    return (word) #now return back, no other processing needed. 
  
  #step2 check if prefix matches ,then remove it 
  for pref in prefixes:
        if word.startswith(pref) and word>pref:  #check the word is only starting with this and is not entire word
            word=re.sub("^{0}".format(pref),"",word,1)
            flag=1
            break #continue with next step 
  #step3 Check the subsitution rules ,if found match ,replace  it .
            
  for key in replacementlist.keys():
        
        if word.endswith(key) and word>key: #check the word is actually end of string and not directly the string. 
          word=re.sub("{0}".format(key),"{0}".format(replacementlist[key]),word,1)
          flag=1
          return (word) #return back now no need to do more stemming ,else wud get overstemmed
         
      
   #step4 Check the suffix , if found match , remove it.          
  
  
  start=len(word)#start checking from list with as per the extra symbols after 3 in source.
  start=start-2
  if start>6 : #as suffix of length 6 and less , so  we limit the start range from 6 
      start=6
  
  for L in  range(start,0,-1): #
            #print("word and L  and orginal word length is:: ",word,L,len(word))
            
            
         #   print("it entered suffix check of L--",L) 
            for suf in suffixes[L]:
               # print("the suffix considered is ",suf)
                if word.endswith(suf):
                    #print("the word was ending with this suffix*****",suf)
                    possibleword=re.sub("{0}$".format(suf),"",word)
                   # print("trimmed word and its length computed is ",possibleword,len(possibleword))
                    if len(possibleword)>=2: #if trimmed word becomes smaller than 2 size ,it might not be proper stem ,so dont use it 
                        word=possibleword #else use the trimmedword as the stemword
                        #print("new word formed after subsitution is ",word)
                        flag=1 #one replacement done ,maybe till end ..then try for other possible .
                        break
                    
            
  
  if flag==0:
     break
 return (word) #return word and flag as not dictionary word 

def guj_stem_to_lemma(datafile,stemmedlist,lemmalist): #to get back the stem to lemma form in the vocabolory found. 
    import io
    import re
    with io.open(datafile,'r+',encoding='utf8') as f:
     lines = f.read()   

    for w in lemmalist: # search is a stemmed version of word in our stemmed list then update the words
     sword=guj_stem_withoutdict(w)
     if sword in stemmedlist and sword!=w:
      #print("the lemma word and stemmed word found in our list are ",w,sword)
      lines=re.sub(r'%s[ ,)\.]'%sword,w+" ",lines)#replace the stemmed with the lemmaform of word at all places infile 
    with io.open(datafile,'w+',encoding='utf8') as w:
      w.write(lines) #update the cleanfile  
    return(datafile)  
def guj_remove_newlines(originalfile,resultfile):
 import io 
 import re
 
 
 with io.open(originalfile,'r+',encoding='utf8') as f:
     lines = f.read() 
 lines=lines.rstrip('\n')
 lines=lines.lstrip('\n')
 lines=lines.replace('\n','')
 with io.open(resultfile,'w+',encoding='utf8') as w:
     w.write(lines)
      
 return(resultfile)  


def guj_corpus_generate(filename):#returns the total lines and all lines in given file
 import io
 #get the individual lines corpora ready 
 corpora=[]
 corpora=guj_sentence_segmenter(filename)
 totalsentences=len(corpora)
 #for s in corpora:
  #  totalsentences=totalsentences+1
 #print("corpus now has sentences --> ",corpora)    
 return (corpora,totalsentences)
     
"""def guj_annaphora_resolve(originalfile,resultfile):#do anaphora resolution and return back the file with all replacements done
 corpus,totsent=guj_corpus_generate(originalfile) #send the stemmed and lemmatized  source file 
 for s in corpus:
     for w in guj_mytokenizer(s):
    
""" 
 