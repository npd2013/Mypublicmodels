{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guj_final_preporcess-v6.py",
      "provenance": [],
      "authorship_tag": "ABX9TyM87l9Yv9dMhJoGo7XrkB7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npd2013/Mypublicmodels/blob/main/guj_final_preporcess_v6_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9tLWo_-QeGu"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\"\"\"guj_preprocess_modules.ipynb\r\n",
        "\r\n",
        "Automatically generated by Colaboratory.\r\n",
        "\r\n",
        "Original file is located at\r\n",
        "    https://colab.research.google.com/drive/1gVCtIAx0l3hMwN1oec77kdT8B0Hf9ErJ\r\n",
        "@author: NIKITA\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "##!cp \"/content/drive/My Drive/Colab Notebooks/guj_final_preporcess-v6.py\" \"guj_final_preporcess.py\" \r\n",
        "# command to include this #file in the RE of colab \r\n",
        "def guj_web_scrap_multiplepage(seed,filename):\r\n",
        " import io\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        " import wikipedia as wk\r\n",
        " wk.set_lang('gu')\r\n",
        " firstpage=wk.page(1)\r\n",
        " try:\r\n",
        "    #firstpage=wk.page(\"મુનશી પ્રેમચદ \")\r\n",
        "    firstpage=wk.page(seed) #start scrapping wiki pages from this seed word\r\n",
        "    data=firstpage.content\r\n",
        "\r\n",
        "   # print('data is',data)\r\n",
        "    with io.open(filename,'a+',encoding='utf8') as f:\r\n",
        "                f.write(data) \r\n",
        " except wk.exceptions.PageError:\r\n",
        "    pass\r\n",
        "\r\n",
        " if firstpage:\r\n",
        "    extralinks=firstpage.links\r\n",
        "   \r\n",
        "\r\n",
        "    for i in extralinks:\r\n",
        "        #if count > 0:\r\n",
        "    \r\n",
        "                try:\r\n",
        "                    newpage=wk.page(i)\r\n",
        "                    data=newpage.content\r\n",
        "                   # count=count-1\r\n",
        "                    with io.open(filename,'a+',encoding='utf8') as f:#append to file if it exists otherwise create new one\r\n",
        "                        f.write(data) \r\n",
        "            \r\n",
        "                except wk.exceptions.PageError:\r\n",
        "                    pass\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        " return(filename)\r\n",
        "              \r\n",
        "                \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"WEB SCRAPPING ::: FETCHING DATA FOR WORKING (FROM WIKIPEDIA)(small file)\"\"\"\r\n",
        "def guj_web_scrap_singlepage(seed,filename):\r\n",
        " import io\r\n",
        " import wikipedia as wk\r\n",
        "\r\n",
        " wk.set_lang('gu')\r\n",
        " firstpage=wk.page(1)\r\n",
        " try:\r\n",
        "\t    firstpage=wk.page(seed) #get information about the given seed word in wiki\r\n",
        "\t    data=firstpage.content\r\n",
        "   #print('data is',data)\r\n",
        "\t    with io.open(filename,'a+',encoding='utf8') as f:\r\n",
        "        \t        f.write(data) \r\n",
        " except wk.exceptions.PageError:\r\n",
        "      return(filename)\r\n",
        "  #print(\"the name returned back is \",datafile)    \r\n",
        " return(filename)\r\n",
        " \r\n",
        "\r\n",
        " \r\n",
        "def guj_clean_html(originalfile,resultfile):\r\n",
        "    #remove the html tags and update the given input file\r\n",
        "  import re\r\n",
        "  import io  \r\n",
        "  \r\n",
        "  with io.open(originalfile,'r',encoding='utf-8') as f: \r\n",
        "       lines=f.read()\r\n",
        "  lines=re.sub('\\<DOCNO\\>.*\\</DOCNO\\>','',lines)# remove the document tags\r\n",
        "  lines=re.sub('\\<.*\\>',' ',lines)#remove all other tags\r\n",
        "  lines=re.sub(r'\\ufeff','',lines) #replace the special encoding code mentioned at start of html files\r\n",
        "  \r\n",
        "  with io.open(resultfile,'w',encoding='utf-8') as w:\r\n",
        "   w.write(lines) \r\n",
        "               \r\n",
        "  return(resultfile)\r\n",
        "  \r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def guj_clean_text(originalfile,resultfile):\r\n",
        "    #replace the inconsistent usage of quotes after sentence end.\r\n",
        " from indicnlp.tokenize import sentence_tokenize\r\n",
        " import re\r\n",
        " import io        \r\n",
        " \r\n",
        " with io.open(originalfile,'r',encoding='utf-8') as f: \r\n",
        "       lines=f.read()\r\n",
        " lines=re.sub(r\"\\n+\",\" \",lines) #remove  extra newlines \r\n",
        " lines=re.sub(r\"!+\",\"!\",lines) #remove multiple exclamations with single\r\n",
        " lines=re.sub(r\"[ ]+\",\" \",lines) #removes extra whitespaces \r\n",
        " lines=re.sub(r\"\\?+\",\"?\",lines) #remove multiple question mark with single\r\n",
        " lines=re.sub(r\"\\.+\",\".\",lines) #replace multiple dot marks with single\r\n",
        " lines=re.sub(r\"\\.\\s*’\",\"’. \",lines)#.Pattern :- start with dot(escape the sp meaning)  followe by zero or more whitespace nd then quotes\r\n",
        " lines=re.sub(r\"\\.\\s*'\",\"'. \",lines)#single quotes handled\r\n",
        " lines=re.sub(r'\\.\\s*\"','\". ',lines)#double quotes handled\r\n",
        " lines=re.sub(r'\\.\\s*”','”. ',lines)#double quotes handled\r\n",
        " lines=re.sub(r\"\\.\\s*’’\",\"’’. \",lines)#single quotes written twice handled\r\n",
        " \r\n",
        " lines=re.sub(r\"\\?\\s*’\",\"’? \",lines)#single quotes handled\r\n",
        " lines=re.sub(r\"\\?\\s*'\",\"'? \",lines)#single quotes handled\r\n",
        " lines=re.sub(r'\\?\\s*\"','\"? ',lines)#double quotes handled\r\n",
        " lines=re.sub(r'\\?\\s*”','”? ',lines)#double quotes handled\r\n",
        " lines=re.sub(r\"\\?\\s*’’\",\"’’? \",lines)#single quotes written twice handled\r\n",
        " \r\n",
        " lines=re.sub(r\"!\\s*’\",\"’! \",lines)#single quotes handled\r\n",
        " lines=re.sub(r\"!\\s*'\",\"'! \",lines)#single quotes handled\r\n",
        " lines=re.sub(r'!\\s*\"','\"! ',lines)#double quotes handled\r\n",
        " lines=re.sub(r'!\\s*”','”! ',lines)#double quotes handled\r\n",
        " lines=re.sub(r\"\\!\\s*’’\",\"’’! \",lines)#single quotes written twice handled\r\n",
        " #list of generally found valid sentence boundary words which can be followd by dot .\r\n",
        " #For others,heuristic of lenght is used.(if length of word having dot as suffix is less than 3 , it is possibly an abbreviation )\r\n",
        " boundarylist=['હતાં','હતા','હતું','હતી','હતો','હતો','ગઈ','ગયો','ગઇ','છું','દે','નહિ','નથી','કરે','થયા','થઈ','થઇ','જશે','થતો','થશે','રહી','છે','છો','છુ','શકે','તે','જા','અહીં','થી','આવ','દો','કર','થાય','પર','જાય','માટે' ,'પરથી','આવ્યું','સુધી','થાય','હતો','થઈ','સાથે','લાગે','હોવા','છતાં','રહેલા','કર્યુ','જુઓ','જાઓ','શકો','નથી','થતો','નહી','હોત','હશે','હોવું','તો','લે']  \r\n",
        " listlines=sentence_tokenize.sentence_split(lines, lang='gu')\r\n",
        " replacedict={}\r\n",
        " for line in listlines: #find all dots for abbreviations .\r\n",
        "   matchfloat=re.search(r'[૧૨૩૪૫૬૭૮૯0o]+\\.',line)#find all the float numbers ,the intermediate dot shud be replaced by *\r\n",
        "   if matchfloat:\r\n",
        "     endpos=matchfloat.end() #get the index in line where the dot has occured in float number\r\n",
        "     actualpos=endpos-1\r\n",
        "     line = line[: actualpos] + '*' + line[actualpos + 1:] # join the previous part with replaced part and then rest of the string\r\n",
        "    # print(\"updated after float replacements \",line)\r\n",
        "   matchingwords=re.findall('[\\S]+[\\.]',line)  #find all prefix with dot at end in one line \r\n",
        "   #print(\"words with abbreviations are \",matchingwords)\r\n",
        "   for m in matchingwords: #for each word having dot at end in the prefix list\r\n",
        "      wordlist=re.split('\\.',m) # get the wordlist having only words,except the dot\r\n",
        "      \r\n",
        "      for w in wordlist:  #  take each word \r\n",
        "       endquotes=[\"'\",'\"',\"’\",\"”\",\" \",\"\"] #if ending with quotes or have blanks then ignore it .\r\n",
        "       if w.endswith(tuple(endquotes)):\r\n",
        "           continue                  \r\n",
        "       if w not in boundarylist : # check is it allowed boundary word then dont modify the subsequent dot\r\n",
        "        if len(w)<3:# Check is length of word found is small then it is  abbreviation\r\n",
        "           originalword=w+'.' #take original word\r\n",
        "           newword=w+'*' #make the replacement word\r\n",
        "           #print(\"new word is \",newword)\r\n",
        "           #print(\"old word is \",originalword)\r\n",
        "          \r\n",
        "           replacedict[originalword]=newword   #maintain the words which are found with abbreviations and their replacement strings with *\r\n",
        " \r\n",
        " \r\n",
        " for key,value in replacedict.items():\r\n",
        "      if key=='.': #as space followed by dot also occurs sometimes, take care to exclude it from replacment\r\n",
        "        continue\r\n",
        "      if key in lines:  #find the word is in lines ,then replace it with word followed by *\r\n",
        "        lines=lines.replace(key,value)\r\n",
        "\r\n",
        " with io.open(resultfile,'w',encoding='utf-8') as w:\r\n",
        "       w.write(lines)\r\n",
        "    \r\n",
        " \r\n",
        " \r\n",
        "               \r\n",
        " return(resultfile)\r\n",
        "\r\n",
        " \r\n",
        " \r\n",
        " \r\n",
        " with io.open(resultfile,'w+',encoding='utf-8') as w:\r\n",
        "       w.write(lines) \r\n",
        "               \r\n",
        " return(resultfile)\r\n",
        "\r\n",
        " \r\n",
        "def guj_stopwordremoval(originalfile,resultfile):\r\n",
        "#need to send the file name containing data to be tokenized. \r\n",
        " import io\r\n",
        " import re\r\n",
        " #static words in stopword list\r\n",
        "\r\n",
        " stoplist=['હતાં','એમ','છે','છો','છુ','હતા','હતું','હતી','હોય',\r\n",
        "'હતો','તેમાં','અને','કે',\r\n",
        "'તથા','તો']\r\n",
        " with io.open(originalfile,'r',encoding='utf-8') as f: \r\n",
        "\t\t  lines=f.read()#get all the lines from file\r\n",
        " sentences,totalsentences=guj_corpus_generate(originalfile)\t\t  \r\n",
        " \r\n",
        " for word in stoplist:\r\n",
        "     lines=re.sub(r\"%s \"%(word), ' ',lines)#for stopword occuring inside line ,so separated by space \r\n",
        "     lines=re.sub(r\"%s\\.\"%(word),'.',lines)#if stopword occurs at end of line ,it would have different sentence end marks following,\r\n",
        "     lines=re.sub(r\"%s\\!\"%(word),'!',lines)# stopwrod can be followed by some special marks , retaint them. \r\n",
        "     lines=re.sub(r\"%s\\)\"%(word),')',lines)\r\n",
        "     lines=re.sub(r\"%s\\,\"%(word),',',lines)\r\n",
        "     lines=re.sub(r\"%s\\?\"%(word),'?',lines)\r\n",
        "     lines=re.sub(r\"%s\\”\"%(word),'”',lines)\r\n",
        "     lines=re.sub(r\"%s\\’\"%(word),'’',lines)\r\n",
        " with io.open(resultfile,'w+',encoding='utf-8') as w:   \r\n",
        "      w.write(lines) \r\n",
        " return(resultfile)    \r\n",
        " \r\n",
        "def guj_makedictionary():\r\n",
        " #returns dictionary made using indowordnet \r\n",
        " import pyiwn\r\n",
        " iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.GUJARATI)\r\n",
        " listwords=iwn.all_words()#all words in wordnet ,store in a list \r\n",
        " return(listwords)\r\n",
        " \r\n",
        "def guj_stemmer_new(originalfile,resultfile,dictionary):\r\n",
        "#using the dictionary to stem\r\n",
        " import io\r\n",
        " import re\r\n",
        " with io.open(originalfile,'r',encoding='utf-8') as f: \r\n",
        "\t\t  lines=f.read()#get all the lines from file\r\n",
        "\r\n",
        " stemmedresultfile='stemmingdata.csv' #to store the original word and stem word found for all words in given file . \r\n",
        " vocablist=guj_makevocab(originalfile) # make vocabulary list of words for given file. \r\n",
        " #to generate the modified file \r\n",
        " with io.open(stemmedresultfile,'a+',encoding='utf-8') as s: #store the stemmed word set ,  with original word\r\n",
        "        for word in vocablist: #take one by one the words and find their stems and do replacment if needed\r\n",
        "          sword=guj_stem_withdict(word,dictionary) #else find stemword and pass the indowordnet also to be used \r\n",
        "               # print('after stemming done, words stemmed and orginal are respectively ----> ',sword,word )\r\n",
        "          \r\n",
        "          if sword!=word:\r\n",
        "              lines=lines.replace(r\"%s\"%(word),r\"%s\"%(sword)) #do replacement of word with stemword \r\n",
        "              s.write(\"{0},{1}\\n\".format(word,sword)) \r\n",
        " with io.open(resultfile,'w',encoding='utf-8') as w: #store the stemmed sentences back in file\r\n",
        "       w.write(lines)\r\n",
        "\r\n",
        " return(resultfile)\r\n",
        "\r\n",
        "def guj_tokenize_with_stemming(originalfile,resultfile):\r\n",
        "#need to send the file name containing data to be tokenized. \r\n",
        "\r\n",
        " import io\r\n",
        " import pyiwn\r\n",
        " import re\r\n",
        " \r\n",
        " with io.open(originalfile,'r',encoding='utf-8') as f: \r\n",
        "\t\t  lines=f.read()#get all the lines from file\r\n",
        "\r\n",
        " iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.GUJARATI)   \r\n",
        " \r\n",
        " stemmedresultfile='stemmingdata.csv' #to store the original word and stem word found for all words in given file . \r\n",
        " vocablist=guj_makevocab(originalfile) # make vocabulary list of words for given file. \r\n",
        " #to generate the modified file \r\n",
        " with io.open(stemmedresultfile,'a+',encoding='utf-8') as s: #store the stemmed word set ,  with original word\r\n",
        "        for word in vocablist: #take one by one the words and find their stems and do replacment if needed\r\n",
        "          sword=guj_stem(word,iwn) #else find stemword and pass the indowordnet also to be used \r\n",
        "               # print('after stemming done, words stemmed and orginal are respectively ----> ',sword,word )\r\n",
        "          \r\n",
        "          if sword!=word:\r\n",
        "              lines=lines.replace(r\"%s\"%(word),r\"%s\"%(sword)) #do replacement of word with stemword \r\n",
        "              s.write(\"{0},{1}\\n\".format(word,sword)) \r\n",
        " with io.open(resultfile,'w',encoding='utf-8') as w: #store the stemmed sentences back in file\r\n",
        "       w.write(lines)\r\n",
        "\r\n",
        " return(resultfile)\r\n",
        "\r\n",
        " \r\n",
        "def guj_norm(originalfile,resultfile): # similar sounding suymbols \r\n",
        "    import io\r\n",
        "    import re\r\n",
        "    with io.open(originalfile,'r',encoding='utf-8') as f: \r\n",
        "\t            lines=f.read()#get all the lines from file\r\n",
        "                 \r\n",
        "    normalizedwords={#frequently inconsistent replacements are directly replaced\r\n",
        "    \"્ઋ\":\"ૃ\",\"ગ્ન\":\"જ્ઞ\",\"ઋ\":\"ૠ\",\"ઑ\": \"ઓ\",':' : '', \"ઇ\":\"ઈ\" , \"જિ\":\"જી\",\"ઊ\":\"ઉ\"}\r\n",
        "    vocablist=guj_makevocab(originalfile) # make vocabulary list of words for given file.\r\n",
        "    for word in vocablist:\r\n",
        "                  #print(\"word obtained for normalizing is \"+word)\r\n",
        "                  for key in normalizedwords.keys():\r\n",
        "                     # print(\"key checked is \"+key)\r\n",
        "                      pos=word.find(key)\r\n",
        "                     \r\n",
        "                      if pos>1:#not at first position then can be updated\r\n",
        "                          nword=re.sub(key,normalizedwords[key],word)#else do the replacment \r\n",
        "                         # print(\"word after  normalizeing \"+nword)\r\n",
        "                          lines=re.sub(word,nword,lines)\r\n",
        "             \r\n",
        "    with io.open(resultfile,'w+',encoding='utf-8') as w: #store the stemmed sentences back in file\r\n",
        "         w.write(lines)\r\n",
        "    \r\n",
        "    return(resultfile)\r\n",
        "    \r\n",
        "def guj_spellcheck(word,dictionary): # check the word in dictionary ,make corrections if needed and return back correct word\r\n",
        "    #returns back word and status , if valid dictinary word returned then status is 1 else 0\r\n",
        "    import io\r\n",
        "    import re\r\n",
        "     \r\n",
        "                 \r\n",
        "    if word in dictionary:\r\n",
        "        return (word,1) #valid word ,so return it back and \"1\" indicates it is valid dictionary word\r\n",
        "    replacementcharacters={#frequently inconsistent replacements are directly replaced\r\n",
        "    \"્ઋ\":\"ૃ\",\"ગ્ન\":\"જ્ઞ\",\"ઋ\":\"ૠ\",\"ઑ\": \"ઓ\",':' : '', \"ઇ\":\"ઈ\" , \"જિ\":\"જી\",\"ઊ\":\"ઉ\",\"િ\":\"ી\",\"ી\":\"િ\",\"ુ\":\"ૂ\",\"ૂ\":\"ુ\"}\r\n",
        "    \r\n",
        "    for key in replacementcharacters.keys():\r\n",
        "                      \r\n",
        "                      if word.find(key):\r\n",
        "                         nword=re.sub(key,replacementcharacters[key],word)#else do the replacment \r\n",
        "                         \r\n",
        "                         if nword in dictionary:\r\n",
        "                              print(\"original word was \",word)\r\n",
        "                              print(\"updated word after  normalizeing \"+nword)\r\n",
        "                              return (nword,1) \r\n",
        "                              \r\n",
        "             \r\n",
        "    return(word,0)#no mathces found in dictionary,so return word back as it is.    \r\n",
        "               \r\n",
        "def guj_sentence_segmenter(originalfile):\r\n",
        " # input is original text file and returns back the list of sentences formed using punkt tokenizer \r\n",
        " from indicnlp.tokenize import indic_tokenize \r\n",
        " from indicnlp.tokenize import sentence_tokenize\r\n",
        " import io\r\n",
        " import re\r\n",
        " with io.open(originalfile,'r',encoding='utf-8') as f: #get the original file content\r\n",
        "         lines=f.read()\r\n",
        " sentences=sentence_tokenize.sentence_split(lines, lang='gu') #divide lines into sentences\r\n",
        " \r\n",
        " #print(\"text received for segmentation is ---> \",sentences)\r\n",
        " return(sentences)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "             \r\n",
        "def guj_mytokenizer(line):\r\n",
        "    #find word tokens in a given line \r\n",
        "\r\n",
        " import io\r\n",
        " tokens=[]\r\n",
        " excludelist=[\"!\",\".\",\"(\",\")\",\"`\",\",\",\"-\",\"?\",'\"','”',\";\",\":\",\"’\",\"'\",'“','”','”',\"‘\",\"“\",\"*\"]\r\n",
        " for word in line.split():#split based on whitespaces the words from given line \r\n",
        "        \r\n",
        "        next=0\r\n",
        "        for symbol in excludelist: \r\n",
        "           if word==symbol: #word is punctuation ,then dont include it as token and check  next word \r\n",
        "              next=1\r\n",
        "              continue\r\n",
        "           pos=word.find(symbol)\r\n",
        "           if pos>-1 and len(word)>1: #symbol found in word and it is not the puntuation symbol\r\n",
        "              word=word.strip(symbol)\r\n",
        "              continue\r\n",
        "           \r\n",
        "        if next==0:\r\n",
        "              tokens.append(word) #current word to be added as token\r\n",
        "              \r\n",
        " \r\n",
        " \r\n",
        " return (tokens)\r\n",
        "             \r\n",
        "def guj_makevocab(datafile):\r\n",
        "  import io\r\n",
        "  mycorpora=[]\r\n",
        "  mycorpora=guj_sentence_segmenter(datafile)\r\n",
        "  #divide lines into sentences\r\n",
        "  words=[]\r\n",
        "  for line in mycorpora:\r\n",
        "     for word in guj_mytokenizer(line):\r\n",
        "        if word not in ['!',\".\",\"(\",\")\",\"`\",\",\",\"-\",\"?\",'\"','”',\";\",\":\",\"’\",\"‘\",\"“\",\"*\",\" \"]: #dont take punctutions in the vocabulory \r\n",
        "          words.append(word)\r\n",
        "  words=set(words) #Get the unique words in entire text \r\n",
        "  vocab={} #store the words found in a dictionary \r\n",
        "  key=0\r\n",
        "  for word in words:\r\n",
        "    vocab[word]=key\r\n",
        "    key=key+1\r\n",
        "  \r\n",
        "  return(vocab)      \r\n",
        "           \r\n",
        "    \r\n",
        " \r\n",
        "def remove_salutes(word):\r\n",
        "     import re\r\n",
        "     if len(word)>5 and word.find(\"સાહેબ\"):\r\n",
        "         word=re.sub(\"સાહેબ\",\"\",word)\r\n",
        "         return (word)\r\n",
        "     if len(word)>4 and word.find(\"બહેન\"):\r\n",
        "         word=re.sub(\"બહેન\",\"\",word)\r\n",
        "         return (word)\r\n",
        "     if len(word)>3 and word.find('ભાઈ') :\r\n",
        "         word=re.sub(\"ભાઈ\",\"\",word)\r\n",
        "         return (word)\r\n",
        "     if len(word)>3 and word.find('ભાઇ') :\r\n",
        "         word=re.sub(\"ભાઇ\",\"\",word)\r\n",
        "         return (word) \r\n",
        "     if len(word)>3 and word.find('બેન') :\r\n",
        "         word=re.sub(\"બેન\",\"\",word)\r\n",
        "         return (word)  \r\n",
        "         \r\n",
        "\r\n",
        "     \r\n",
        "            \r\n",
        "def guj_stem_withdict(word,dictionary):      \r\n",
        " import io\r\n",
        " import re\r\n",
        " listwords=dictionary#all words in wordnet\r\n",
        " word=remove_salutes(word)#words with bhai or bahen or saheb attached are stripped \r\n",
        " dword,status=guj_spellcheck(word,dictionary)#check the word in dictionary,if found equivalent word ,return that as replacment word\r\n",
        " if status==1:\r\n",
        "     return dword #found dictionary word ,return it \r\n",
        "      \r\n",
        " suffixes ={  1: [\"ો\", \"ી\" ,\" ે\", \"ા\" , \"ૂ\", \"ે\" , \"ૢ\", \"ૄ\" ,\"ૈ\" , \"ૌ\"  ,\"ુ\",\"્\",\"ઈ\",\"એ\",\"ઇ\"],\r\n",
        "   2: [\"ઓએ\",\"ીઓ\" ,\"ીએ\",\"ોએ\" ,\"ાઓ\",\"ાઈ\",\"ાઇ\",\"થી\",\"ના\",\"ની\",\"ને\",\"નો\" ,\"નુ\",\"મા\",\"મી\",\"મુ\",\"શે\" ,\"ીશ\",\"શો\",\"મત\",\"ઓ\",\"એ\" ,\"ું\",\"ંુ\",\"ાં\",\"ંા\",\"ંૂ\", \"ૂં\",\r\n",
        "\"તો\",\"તી\",\"તા\",\"યા\",\"યુ\",\"જે\",\"ાય\",\"યો\",\"કે\",\"જો\",\"જી\",\"ેલ\",\"વા\",\"વી\"] ,\r\n",
        "            3: [\"ાઓએ\",\"ઓના\", \"ઓનુ\",\"ઓમા\",\"ઓનો\",\"ઓની\",\"ઓને\",\"ઓથી\",\"ઓના\",\"નું\",\"નાં\",\"માં\",\"તાં\",\"યું\",\"યાં\",\"તું\",\"મું\",\"વાં\",\"ોને\" ,\"ોની\" ,\"ાના\",\"ાની\" ,\"ીને\",\r\n",
        "\"ાનો\" ,\"ોનુ\",\"માન\",\"ીતા\", \"કાળ\",\"ોથી\" ,\"દાર\", \"જનો\",\"ાવે\",\"ાવી\",\"ાવર\",\"વું\",\"ેથી\",\"પણુ\",\"ેલી\",\"ેલા\",\"ેલોરી\",\"નાર\"],\r\n",
        "            4: [ \"ીઓની\",\"ઓનું\",\"ઓનાં\",\"ઓમાં\",\"ાઓની\" ,\"ોઓથી\" , \"ાઓનો\", \"ાઓને\",\"ાઓનુ\",\"ોનાં\" ,\"ોનું\" ,\"ાનાં\", \"ાનું\", \"વાળા\",\"વાળી\",\"પૂ઼ણઁ\",\"વેરા\",\"વાળો\",\"પણું\",\r\n",
        "\"માથી\",\"ાવીશ\",\r\n",
        "\"વાથી\",\"વાના\",\"વાનો\",\"વાની\",\"નારી\",\"નારો\",\"ીશુ\",\"ેલાં\",\"ેલું\"] , \r\n",
        "            5: [\"ાઓનું\",\"માંથી\" , \"વાનાં\",\"ીશું\",\"પૂર્વક\" ,\"વામાં\",\"વાનું\" ,\"વાળું\",\"વવાની\",\"વવાનો\",\"યાનાં\"],\r\n",
        "   6:[ \"વાળાએ\",\"વાળાઓની\", \"વાળાઓનું\",\"વાળાઓનુ\", \"વાળાની\" , \"વાળાનો\",\"વાવાળા\",\"ોમાંથી\"  ]\r\n",
        "   }\r\n",
        "\r\n",
        " prefixes =   [\"બિન\",\"ગેર\",\"સર્વ\", \"અર્ધ\"]\r\n",
        " replacementlist=    {\"ન્યાં\" :\"ન\",\"ન્યા\" :\"ન\",\r\n",
        "\"ન્યાના\": \"ન\",\"ન્યાની\":\"ન\",\"ન્યાનો\":\"ન\",\"ન્યા\" :\"ન\",\r\n",
        "\"ન્યું\":\"ન\", \"ન્યુ\":\"ન\", \"ન્યો\" :\"ન\", \"વ્યાનાં\" : \"વ\", \"વ્યાના\" : \"વ\",\"વ્યાની\" : \"વ\", \"વ્યો\": \"વ\",\r\n",
        "\"વ્યાં\" :\"વ\",\"વ્યા\":\"વ\", \"વ્યાના\":\"વ\", \"વ્યાનું\":\"વ\", \"વ્યાનુ\":\"વ\",  \"વ્યાનો\" : \"વ\",\r\n",
        "  \"વ્યુ\":\"વ\",\"વ્યે\" :\"વ\", \"પ્યા\": \"પ\",\r\n",
        "\"પ્યું\":\"પ\", \"પ્યુ\":\"પ\", \"પ્યો\":\"પ\" , \"પ્યાં\":\"પ\", \"પ્યા\":\"પ\",\"પ્યાની\": \"પ\",\"પ્યાનો\": \"પ\",\r\n",
        "\"પ્યુ\" :\"પ\",\"ખ્યાનું\":\"ખ\", \"ખ્યાનુ\":\"ખ\",\"ખ્યા\":\"ખ\", \"ખ્યું\":\"ખ\",\"ખ્યુ\":\"ખ\",\"ખ્યાં\" :\"ખ\",\"ખ્યા\" :\"ખ\",\r\n",
        "\"ખ્યાના\":\"ખ\",\"ખ્યો\":\"ખ\",\"ળ્યા\":\"ળ\",\"ળ્યું\" :\"ળ\",\"ળ્યુ\" :\"ળ\",\"ળ્યાં\" :\"ળ\",\"ળ્યા\" :\"ળ\",\r\n",
        "\"ળ્યો\": \"ળ\",\"લ્યાં\" : \"લ\",\"લ્યા\":\"લ\",\"લ્યું\" :\"લ\",\"લ્યુ\":\"લ\",\"લ્યો\":\"લ\",\r\n",
        "\"લ્યે\": \"લ\",\"વવુ\":\"વ\",\"વવું\":\"વ\",\"રું\":\"ર\",\"વવો\":\"વ\"\r\n",
        "\r\n",
        "  }  # Extra similar replacements added to match the words without ending anusvar which are removed in normalizing\r\n",
        " \r\n",
        " while True:#infinite loop ,to continue for currentword only stop when no updation done \r\n",
        "  flag=0\r\n",
        "  #print(\"in process for stemming word - \",word)\r\n",
        "  #step1 Check if a number ,then remove the extra word inflection at end \r\n",
        "  number=[\"૧\",\"૨\",\"૩\",\"૪\",\"૫\",\"૬\",\"૭\",\"૮\",\"૯\",\"0\",\"૦\"]\r\n",
        "  if word.startswith(tuple(number)): #check is the given word a number? \r\n",
        "    regex = re.compile('[^૧૨૩૪૫૬૭૮૯0૦]')#if yes then remove its non numeric suffix part\r\n",
        "    word=regex.sub('',word)\r\n",
        "    return word #now return back, no other processing needed. \r\n",
        "  \r\n",
        "  #step2 check if prefix matches ,then remove it \r\n",
        "  for pref in prefixes:\r\n",
        "        if word.startswith(pref) and word>pref:  #check the word is only starting with this and is not entire word\r\n",
        "            word=re.sub(\"^{0}\".format(pref),\"\",word,1)\r\n",
        "            flag=1\r\n",
        "            break #continue with next step \r\n",
        "  #step3 Check the subsitution rules ,if found match ,replace  it .\r\n",
        "            \r\n",
        "  for key in replacementlist.keys():\r\n",
        "        \r\n",
        "        if word.endswith(key) and word>key: #check the word is actually end of string and not directly the string. \r\n",
        "          word=re.sub(\"{0}\".format(key),\"{0}\".format(replacementlist[key]),word,1)\r\n",
        "          flag=1\r\n",
        "          return word #return back now no need to do more stemming ,else wud get overstemmed\r\n",
        "         \r\n",
        "      \r\n",
        "   #step4 Check the suffix , if found match , remove it.          \r\n",
        "  \r\n",
        "  \r\n",
        "  start=len(word)#start checking from list with as per the extra symbols after 3 in source.\r\n",
        "  start=start-2\r\n",
        "  if start>6 : #as suffix of lenght 6 and less , so  we limit the start range from 6 \r\n",
        "      start=6\r\n",
        "  for L in  range(start,0,-1): #\r\n",
        "           # print(\"word and L  and orginal word length is:: \",word,L,len(word))\r\n",
        "            \r\n",
        "            if word.endswith(\"ં\"):#the ending anusvar is used inconsistently ,so remove it. \r\n",
        "                word=re.sub(\"ં\",\"\",word)\r\n",
        "         #   print(\"it entered suffix check of L--\",L) \r\n",
        "            for suf in suffixes[L]:\r\n",
        "               # print(\"the suffix considered is \",suf)\r\n",
        "                if word.endswith(suf):\r\n",
        "                  #  print(\"the word was ending with this suffix*****\",suf)\r\n",
        "                    possibleword=re.sub(\"{0}$\".format(suf),\"\",word)\r\n",
        "                   # print(\"trimmed word and its length computed is \",possibleword,len(possibleword))\r\n",
        "                    if len(possibleword)>=2: #if trimmed word becomes smaller than 2 size ,it might not be proper stem ,so dont use it \r\n",
        "                        word=possibleword #else use the trimmedword as the stemword\r\n",
        "                        #print(\"new word formed after subsitution is \",word)\r\n",
        "                        flag=1 #one replacement done ,maybe till end ..then try for other possible .\r\n",
        "                        break\r\n",
        "                    \r\n",
        "            if flag==1:\r\n",
        "              dword,status=guj_spellcheck(word,dictionary)#check the word in dictionary,if found equivalent word ,return that as replacment word\r\n",
        "              if status==1:\r\n",
        "                 return dword #found dictionary word ,return it \r\n",
        "              \r\n",
        "  \r\n",
        "  if flag==0:\r\n",
        "     break\r\n",
        " return word\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def guj_stem(word,iwn):\r\n",
        " #using all possible words in list \r\n",
        " import pyiwn      \r\n",
        " import io\r\n",
        " import re\r\n",
        " listwords=iwn.all_words()#all words in wordnet ,store in a list \r\n",
        " if word in listwords:\r\n",
        "     return word #word a valid word in indowordnet dictionary ,so return back without continuing\r\n",
        " suffixes ={  1: [\"ો\", \"ી\" ,\" ે\", \"ા\" , \"ૂ\", \"ે\" , \"ૢ\", \"ૄ\" ,\"ૈ\" , \"ૌ\"  ,\"ુ\",\"્\",\"ઈ\",\"એ\",\"ઇ\"],\r\n",
        "   2: [\"ઓએ\",\"ીઓ\" ,\"ીએ\",\"ોએ\" ,\"ાઓ\",\"ાઈ\",\"ાઇ\",\"થી\",\"ના\",\"ની\",\"ને\",\"નો\" ,\"નુ\",\"મા\",\"મી\",\"મુ\",\"શે\" ,\"ીશ\",\"શો\",\"મત\",\"ઓ\",\"એ\" ,\"ું\",\"ંુ\",\"ાં\",\"ંા\",\"ંૂ\", \"ૂં\",\r\n",
        "\"તો\",\"તી\",\"તા\",\"યા\",\"યુ\",\"જે\",\"ાય\",\"યો\",\"કે\",\"જો\",\"જી\",\"ેલ\",\"વા\",\"વી\"] ,\r\n",
        "            3: [\"ાઓએ\",\"ઓના\", \"ઓનુ\",\"ઓમા\",\"ઓનો\",\"ઓની\",\"ઓને\",\"ઓથી\",\"ઓના\",\"નું\",\"નાં\",\"માં\",\"તાં\",\"યું\",\"યાં\",\"તું\",\"મું\",\"વાં\",\"ોને\" ,\"ોની\" ,\"ાના\",\"ાની\" ,\"ીને\",\r\n",
        "\"ાનો\" ,\"ોનુ\",\"માન\",\"ીતા\", \"કાળ\",\"ોથી\" ,\"દાર\", \"જનો\",\"ાવે\",\"ાવી\",\"ાવર\",\"વું\",\"ેથી\",\"પણુ\",\"ભાઈ\",\"ભાઇ\",\"ેલી\",\"ેલા\",\"ેલોરી\",\"નાર\",\"બેન\"],\r\n",
        "            4: [ \"ીઓની\",\"ઓનું\",\"ઓનાં\",\"ઓમાં\",\"ાઓની\" ,\"ોઓથી\" , \"ાઓનો\", \"ાઓને\",\"ાઓનુ\",\"ોનાં\" ,\"ોનું\" ,\"ાનાં\", \"ાનું\", \"વાળા\",\"વાળી\",\"પૂ઼ણઁ\",\"વેરા\",\"વાળો\",\"પણું\",\r\n",
        "\"માથી\",\"ાવીશ\",\r\n",
        "\"બહેન\",\"વાથી\",\"વાના\",\"વાનો\",\"વાની\",\"નારી\",\"નારો\",\"ીશુ\",\"ેલાં\",\"ેલું\"] , \r\n",
        "            5: [\"ાઓનું\",\"માંથી\" , \"વાનાં\",\"ીશું\",\"પૂર્વક\" ,\"વામાં\",\"વાનું\" ,\"વાળું\",\"વવાની\",\"વવાનો\",\"યાનાં\"],\r\n",
        "   6:[ \"વાળાએ\",\"વાળાઓની\", \"વાળાઓનું\",\"વાળાઓનુ\", \"વાળાની\" , \"વાળાનો\",\"વાવાળા\",\"ોમાંથી\"]\r\n",
        "   }\r\n",
        "\r\n",
        " prefixes =   [\"બિન\",\"ગેર\",\"સર્વ\", \"અર્ધ\"]\r\n",
        " replacementlist=    {\"ન્યાં\" :\"ન\",\"ન્યા\" :\"ન\",\r\n",
        "\"ન્યાના\": \"ન\",\"ન્યાની\":\"ન\",\"ન્યાનો\":\"ન\",\"ન્યા\" :\"ન\",\r\n",
        "\"ન્યું\":\"ન\", \"ન્યુ\":\"ન\", \"ન્યો\" :\"ન\", \"વ્યાનાં\" : \"વ\", \"વ્યાના\" : \"વ\",\"વ્યાની\" : \"વ\", \"વ્યો\": \"વ\",\r\n",
        "\"વ્યાં\" :\"વ\",\"વ્યા\":\"વ\", \"વ્યાના\":\"વ\", \"વ્યાનું\":\"વ\", \"વ્યાનુ\":\"વ\",  \"વ્યાનો\" : \"વ\",\r\n",
        "  \"વ્યુ\":\"વ\",\"વ્યે\" :\"વ\", \"પ્યા\": \"પ\",\r\n",
        "\"પ્યું\":\"પ\", \"પ્યુ\":\"પ\", \"પ્યો\":\"પ\" , \"પ્યાં\":\"પ\", \"પ્યા\":\"પ\",\"પ્યાની\": \"પ\",\"પ્યાનો\": \"પ\",\r\n",
        "\"પ્યુ\" :\"પ\",\"ખ્યાનું\":\"ખ\", \"ખ્યાનુ\":\"ખ\",\"ખ્યા\":\"ખ\", \"ખ્યું\":\"ખ\",\"ખ્યુ\":\"ખ\",\"ખ્યાં\" :\"ખ\",\"ખ્યા\" :\"ખ\",\r\n",
        "\"ખ્યાના\":\"ખ\",\"ખ્યો\":\"ખ\",\"ળ્યા\":\"ળ\",\"ળ્યું\" :\"ળ\",\"ળ્યુ\" :\"ળ\",\"ળ્યાં\" :\"ળ\",\"ળ્યા\" :\"ળ\",\r\n",
        "\"ળ્યો\": \"ળ\",\"લ્યાં\" : \"લ\",\"લ્યા\":\"લ\",\"લ્યું\" :\"લ\",\"લ્યુ\":\"લ\",\"લ્યો\":\"લ\",\r\n",
        "\"લ્યે\": \"લ\",\"વવુ\":\"વ\",\"વવું\":\"વ\",\"રું\":\"ર\",\"વવો\":\"વ\"\r\n",
        "\r\n",
        "  }  # Extra similar replacements added to match the words without ending anusvar which are removed in normalizing\r\n",
        " \r\n",
        " while True:#infinite loop ,to continue for currentword only stop when no updation done \r\n",
        "  flag=0\r\n",
        "  #print(\"in process for stemming word - \",word)\r\n",
        "  #step1 Check if a number ,then remove the extra word inflection at end \r\n",
        "  number=[\"૧\",\"૨\",\"૩\",\"૪\",\"૫\",\"૬\",\"૭\",\"૮\",\"૯\",\"0\",\"૦\"]\r\n",
        "  if word.startswith(tuple(number)): #check is the given word a number? \r\n",
        "    regex = re.compile('[^૧૨૩૪૫૬૭૮૯0૦]')#if yes then remove its non numeric suffix part\r\n",
        "    word=regex.sub('',word)\r\n",
        "    return word #now return back, no other processing needed. \r\n",
        "  \r\n",
        "  #step2 check if prefix matches ,then remove it \r\n",
        "  for pref in prefixes:\r\n",
        "        if word.startswith(pref) and word>pref:  #check the word is only starting with this and is not entire word\r\n",
        "            word=re.sub(\"^{0}\".format(pref),\"\",word,1)\r\n",
        "            flag=1\r\n",
        "            break #continue with next step \r\n",
        "  #step3 Check the subsitution rules ,if found match ,replace  it .\r\n",
        "            \r\n",
        "  for key in replacementlist.keys():\r\n",
        "        \r\n",
        "        if word.endswith(key) and word>key: #check the word is actually end of string and not directly the string. \r\n",
        "          word=re.sub(\"{0}\".format(key),\"{0}\".format(replacementlist[key]),word,1)\r\n",
        "          flag=1\r\n",
        "          return word #return back now no need to do more stemming ,else wud get overstemmed\r\n",
        "         \r\n",
        "      \r\n",
        "   #step4 Check the suffix , if found match , remove it.          \r\n",
        "  \r\n",
        "  if word.endswith(\"ં\"):#the ending anusvar is used inconsistently ,so remove it. \r\n",
        "                word=re.sub(\"ં\",\"\",word)\r\n",
        "  start=len(word)#start checking from list with as per the extra symbols after 3 in source.\r\n",
        "  start=start-2\r\n",
        "  if start>6 : #as suffix of lenght 6 and less , so  we limit the start range from 6 \r\n",
        "      start=6\r\n",
        "  for L in  range(start,0,-1):\r\n",
        "           # print(\"word and L  and orginal word length is:: \",word,L,len(word))\r\n",
        "            \r\n",
        "            if word.endswith(\"ં\"):#the ending anusvar is used inconsistently ,so remove it. \r\n",
        "                word=re.sub(\"ં\",\"\",word)\r\n",
        "         #   print(\"it entered suffix check of L--\",L) \r\n",
        "            for suf in suffixes[L]:\r\n",
        "               # print(\"the suffix considered is \",suf)\r\n",
        "                if word.endswith(suf):\r\n",
        "                  #  print(\"the word was ending with this suffix*****\",suf)\r\n",
        "                    possibleword=re.sub(\"{0}$\".format(suf),\"\",word)\r\n",
        "                   # print(\"trimmed word and its length computed is \",possibleword,len(possibleword))\r\n",
        "                    if len(possibleword)>=3: #if trimmed word becomes smaller than 2 size ,it might not be proper stem ,so dont use it \r\n",
        "                        word=possibleword #else use the trimmedword as the stemword\r\n",
        "                        #print(\"new word formed after subsitution is \",word)\r\n",
        "                        flag=1 #one replacement done ,maybe till end ..then try for other possible .\r\n",
        "                        break\r\n",
        "                    \r\n",
        "            if flag==1:\r\n",
        "              if word in listwords: #check is it now in dictionary form ,after stemming ,then also return back  \r\n",
        "                return word\r\n",
        "              break # come out of this same loop also and take the word and try again for further stemming\r\n",
        "  \r\n",
        "  if flag==0:\r\n",
        "     break\r\n",
        " return word\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "               \r\n",
        "\r\n",
        "\r\n",
        "def guj_remove_newlines(originalfile,resultfile):\r\n",
        " import io \r\n",
        " import re\r\n",
        " \r\n",
        " \r\n",
        " with io.open(originalfile,'r+',encoding='utf8') as f:\r\n",
        "     lines = f.read() \r\n",
        " lines=lines.rstrip('\\n')\r\n",
        " lines=lines.lstrip('\\n')\r\n",
        " lines=lines.replace('\\n','')\r\n",
        " with io.open(resultfile,'w+',encoding='utf8') as w:\r\n",
        "     w.write(lines)\r\n",
        "      \r\n",
        " return(resultfile)  \r\n",
        "\r\n",
        "\r\n",
        "def guj_corpus_generate(filename):#returns the total lines and all lines in given file\r\n",
        " import io\r\n",
        " #get the individual lines corpora ready \r\n",
        " corpora=[]\r\n",
        " corpora=guj_sentence_segmenter(filename)\r\n",
        " totalsentences=0\r\n",
        " for s in corpora:\r\n",
        "    totalsentences=totalsentences+1\r\n",
        " #print(\"corpus now has sentences --> \",corpora)    \r\n",
        " return (corpora,totalsentences)\r\n",
        "     "
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}