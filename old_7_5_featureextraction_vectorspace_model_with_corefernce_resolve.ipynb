{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "old 7.5-featureextraction - vectorspace model with corefernce resolve",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tZ3-PoOPE4KUlgX4AZqserM5PeV-ga3W",
      "authorship_tag": "ABX9TyPn82E/ZTjY1ocu8PJSigJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npd2013/Mypublicmodels/blob/main/old_7_5_featureextraction_vectorspace_model_with_corefernce_resolve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2jwlnJOe0wa"
      },
      "source": [
        "SETUP1:-  \n",
        "-Single document text summarization \n",
        "\n",
        "-frequency based method for scoring sentences \n",
        "\n",
        "-preprocessing applied are knowledge based normalization,  stopword removal and stemming.\n",
        "-Using  CUE WORDS - actors in story.\n",
        "\n",
        "-automatic evaluation using rouge and compression ratio analysis.\n",
        "\n",
        "note:results stored in another file \n",
        "\n",
        "\n",
        "Algorithm steps :- \n",
        "\n",
        "1) Preprocessing of text- \n",
        "\n",
        "    1.1)Cleaning \n",
        "\n",
        "    1.1.1)Removal of unnecessary symbols/charachters like html tags ,extra spaces,extra newlines etc.\n",
        "         \n",
        " \n",
        " 1.2) Line segmentation\n",
        " \n",
        "    -using indicnlp with own segmenter  \n",
        "    -Divide text into lines and find token words using own word segmenter.\n",
        " 1.3) word segmentation\n",
        "    -Divide the lines into words.\n",
        "    -words can be separated by whitespace or characters like quotes or comma.  use the punctuation list to exclude from the word.Only extract valid words \n",
        "\n",
        "  1.4) Stopword removal \n",
        " \n",
        "       -basic set of 15 words removed from lines. \n",
        "  1.5) Co-reference resolution\n",
        "       - if current sentences contain referencewords\n",
        "            - if previous two sentences contain \n",
        "               \"actor\" words or its variations\n",
        "                - replace referenceword with actor word.\n",
        "\n",
        "   \n",
        " 1.5) Stemming\n",
        "    -if word hyphenated (considered as joint word)\n",
        "        -exit   \n",
        "\n",
        "    -use static dictionary of standard words.\n",
        " \n",
        "    -remove matching salutation from suffix word.\n",
        "    -Check is it valid dictionary word or wrong spelled dictionary word \n",
        "       -add the word in lemmalist\n",
        "       exit\n",
        "    \n",
        "    -if not valid dictionary word\n",
        "\n",
        "       -look up prefix and trim if match found\n",
        "\n",
        "       -look up replacement and trim if match found\n",
        "\n",
        "       - look up longest suffix rules matching and trim if match found.\n",
        "    \n",
        "    -if trimmed word size>=3 \n",
        "          - stemword = trimword\n",
        "          - store the mapping of stemword and orginalword in lemmalist.\n",
        "          - replace word with stemword found\n",
        "  \n",
        "1.6) Stem to lemma\n",
        "   -for all words in lemmalist \n",
        "     - if \n",
        "                   \n",
        "               \n",
        "       \n",
        "\n",
        "2)Make vector space representation -based on term frequency\n",
        "\n",
        "    2.1) Compute tfidf using sklearn module of python.\n",
        "\n",
        "    2.2) For each sentence - \n",
        "\n",
        "     2.2.1) Compute sentence score= total tfidf score of all words in sentence divide by number of words in sentence.\n",
        "     2.2.2) If it is first or last sentence , increase its weightage ten times.\n",
        "  \n",
        "  2.3) For each sentence -\n",
        "\n",
        "     3.2.1) Compute the average normalized score  : \n",
        "          Current sentence score divide by total sentence score of text.\n",
        "      \n",
        "\n",
        " 4) //Find top \"n\" scored sentences.\n",
        "\n",
        " 4.1) Set n= number of lines in gold summary. 0r \"x\" percent of original text.    \n",
        " 4.2) Summary_length=n\n",
        " 4.3) Sorted_sentences=sort(sentences) based on their scores.\n",
        "    \n",
        " 5) //Generate the summary -\n",
        "\n",
        "    for sentences = 1 to gold_summary_length \n",
        "      - if sentence in gold_summary  \n",
        "           put sentence in system_summary. \n",
        "  \n",
        "6)// Evaluate the summary-\n",
        "\n",
        "    6.1) use the Rouge score \n",
        "      - Get the precision ,recall and F-measure for rouge-1 ,rouge2 and rouge-LCS.   \n",
        "\n",
        "    6.2) Find comression ratio of text\n",
        "       6.2.1) sentence compression ratio\n",
        "        = total sentences in system_summary/total sentences in source text \n",
        "        6.2.2) Word compression ratio\n",
        "        =total words in system_summary/total words in source \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeLGOonUpR4z",
        "outputId": "0eefa7a1-852b-4a34-9bdf-730bf4b053cc"
      },
      "source": [
        "#install needed packages \n",
        "#!pip install  gensim  #has word2vec model built, using english \n",
        "\n",
        "!pip install pyiwn\n",
        "!pip install -U scikit-learn\n",
        "!pip install rouge\n",
        "!pip install indic-nlp-library\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyiwn\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/80/55d7412ed602071de200b74d75f7cba8c9b4fe6a2f3df33c6d5cd0e6cb83/pyiwn-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyiwn) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyiwn) (1.1.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyiwn) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pyiwn) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pyiwn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyiwn) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pyiwn) (1.15.0)\n",
            "Installing collected packages: pyiwn\n",
            "Successfully installed pyiwn-0.0.5\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 128kB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting indic-nlp-library\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/d4/495bb43b88a2a6d04b09c29fc5115f24872af74cd8317fe84026abd4ddb1/indic_nlp_library-0.81-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.19.5)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/06/2b/dfad6a1831c3aeeae25d8d3d417224684befbf45e10c7f2141631616a6ed/sphinx-argparse-0.2.5.tar.gz\n",
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Collecting sphinx-rtd-theme\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/24/2475e8f83519b54b2148d4a56eb1111f9cec630d088c3ffc214492c12107/sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2MB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.5)\n",
            "Collecting docutils<0.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/44/8a15e45ffa96e6cf82956dd8d7af9e666357e16b0d93b253903475ee947f/docutils-0.16-py2.py3-none-any.whl (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.0.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (20.9)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Building wheels for collected packages: sphinx-argparse\n",
            "  Building wheel for sphinx-argparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sphinx-argparse: filename=sphinx_argparse-0.2.5-cp37-none-any.whl size=11552 sha256=521265843178d05fd72126d27a2054ea216fc65e9257db6573e435eb2a1d19c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/18/1b/4990a1859da4edc77ab312bc2986c08d2733fb5713d06e44f5\n",
            "Successfully built sphinx-argparse\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sphinx-argparse, morfessor, docutils, sphinx-rtd-theme, indic-nlp-library\n",
            "  Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "Successfully installed docutils-0.16 indic-nlp-library-0.81 morfessor-2.0.6 sphinx-argparse-0.2.5 sphinx-rtd-theme-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XlBVjZMvCPA",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "619c7396-9322-42ec-918f-a4fed0bbae43"
      },
      "source": [
        "from google.colab import files #upload the files from local machine on the colab environment \n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e53a00a6-ab7b-41aa-952c-4d4083fec538\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e53a00a6-ab7b-41aa-952c-4d4083fec538\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving story1.txt to story1.txt\n",
            "Saving story1-gold.txt to story1-gold.txt\n",
            "Saving story1-topic.txt to story1-topic.txt\n",
            "Saving story2.txt to story2.txt\n",
            "Saving story2-gold.txt to story2-gold.txt\n",
            "Saving story2-topic.txt to story2-topic.txt\n",
            "Saving story3.txt to story3.txt\n",
            "Saving story3-gold.txt to story3-gold.txt\n",
            "Saving story3-topic.txt to story3-topic.txt\n",
            "Saving story4.txt to story4.txt\n",
            "Saving story4-gold.txt to story4-gold.txt\n",
            "Saving story4-topic.txt to story4-topic.txt\n",
            "Saving story5.txt to story5.txt\n",
            "Saving story5-gold.txt to story5-gold.txt\n",
            "Saving story5-topic.txt to story5-topic.txt\n",
            "Saving story6.txt to story6.txt\n",
            "Saving story6-gold.txt to story6-gold.txt\n",
            "Saving story6-topic.txt to story6-topic.txt\n",
            "Saving story7.txt to story7.txt\n",
            "Saving story7-gold.txt to story7-gold.txt\n",
            "Saving story7-topic.txt to story7-topic.txt\n",
            "Saving story8.txt to story8.txt\n",
            "Saving story8-gold.txt to story8-gold.txt\n",
            "Saving story8-topic.txt to story8-topic.txt\n",
            "Saving story9.txt to story9.txt\n",
            "Saving story9-gold.txt to story9-gold.txt\n",
            "Saving story9-topic.txt to story9-topic.txt\n",
            "Saving story10.txt to story10.txt\n",
            "Saving story10-gold.txt to story10-gold.txt\n",
            "Saving story10-topic.txt to story10-topic.txt\n",
            "Saving story11.txt to story11.txt\n",
            "Saving story11-gold.txt to story11-gold.txt\n",
            "Saving story11-topic.txt to story11-topic.txt\n",
            "Saving story12.txt to story12.txt\n",
            "Saving story12-gold.txt to story12-gold.txt\n",
            "Saving story12-topic.txt to story12-topic.txt\n",
            "Saving story13.txt to story13.txt\n",
            "Saving story13-gold.txt to story13-gold.txt\n",
            "Saving story13-topic.txt to story13-topic.txt\n",
            "Saving story14.txt to story14.txt\n",
            "Saving story14-gold.txt to story14-gold.txt\n",
            "Saving story14-topic.txt to story14-topic.txt\n",
            "Saving story15.txt to story15.txt\n",
            "Saving story15-gold.txt to story15-gold.txt\n",
            "Saving story15-topic.txt to story15-topic.txt\n",
            "Saving story16.txt to story16.txt\n",
            "Saving story16-gold.txt to story16-gold.txt\n",
            "Saving story16-topic.txt to story16-topic.txt\n",
            "Saving story17.txt to story17.txt\n",
            "Saving story17-gold.txt to story17-gold.txt\n",
            "Saving story17-topic.txt to story17-topic.txt\n",
            "Saving story18.txt to story18.txt\n",
            "Saving story18-gold.txt to story18-gold.txt\n",
            "Saving story18-topic.txt to story18-topic.txt\n",
            "Saving story19.txt to story19.txt\n",
            "Saving story19-topic.txt to story19-topic.txt\n",
            "Saving story20.txt to story20.txt\n",
            "Saving story20-gold.txt to story20-gold.txt\n",
            "Saving story20-topic.txt to story20-topic.txt\n",
            "Saving story21.txt to story21.txt\n",
            "Saving story21-gold.txt to story21-gold.txt\n",
            "Saving story21-topic.txt to story21-topic.txt\n",
            "Saving story22.txt to story22.txt\n",
            "Saving story22-gold.txt to story22-gold.txt\n",
            "Saving story22-topic.txt to story22-topic.txt\n",
            "Saving story23.txt to story23.txt\n",
            "Saving story23-gold.txt to story23-gold.txt\n",
            "Saving story23-topic.txt to story23-topic.txt\n",
            "Saving story24.txt to story24.txt\n",
            "Saving story24-gold.txt to story24-gold.txt\n",
            "Saving story24-topic.txt to story24-topic.txt\n",
            "Saving story25.txt to story25.txt\n",
            "Saving story25-gold.txt to story25-gold.txt\n",
            "Saving story25-topic.txt to story25-topic.txt\n",
            "Saving story26.txt to story26.txt\n",
            "Saving story26-gold.txt to story26-gold.txt\n",
            "Saving story26-topic.txt to story26-topic.txt\n",
            "Saving story27.txt to story27.txt\n",
            "Saving story27-gold.txt to story27-gold.txt\n",
            "Saving story27-topic.txt to story27-topic.txt\n",
            "Saving story28.txt to story28.txt\n",
            "Saving story28-gold.txt to story28-gold.txt\n",
            "Saving story28-topic.txt to story28-topic.txt\n",
            "Saving story29.txt to story29.txt\n",
            "Saving story29-gold.txt to story29-gold.txt\n",
            "Saving story29-topic.txt to story29-topic.txt\n",
            "Saving story30.txt to story30.txt\n",
            "Saving story30-gold.txt to story30-gold.txt\n",
            "Saving story30-topic.txt to story30-topic.txt\n",
            "Saving story31.txt to story31.txt\n",
            "Saving story31-gold.txt to story31-gold.txt\n",
            "Saving story31-topic.txt to story31-topic.txt\n",
            "Saving story32.docx to story32.docx\n",
            "Saving story32.txt to story32.txt\n",
            "Saving story32-gold.txt to story32-gold.txt\n",
            "Saving story32-topic.txt to story32-topic.txt\n",
            "Saving story33.txt to story33.txt\n",
            "Saving story33-gold.txt to story33-gold.txt\n",
            "Saving story33-topic.txt to story33-topic.txt\n",
            "Saving story34.txt to story34.txt\n",
            "Saving story34-gold.txt to story34-gold.txt\n",
            "Saving story34-topic.txt to story34-topic.txt\n",
            "Saving story35.txt to story35.txt\n",
            "Saving story35-gold.txt to story35-gold.txt\n",
            "Saving story35-topic.txt to story35-topic.txt\n",
            "Saving story36.txt to story36.txt\n",
            "Saving story36-gold.txt to story36-gold.txt\n",
            "Saving story36-topic.txt to story36-topic.txt\n",
            "Saving story37.txt to story37.txt\n",
            "Saving story37-gold.txt to story37-gold.txt\n",
            "Saving story37-topic.txt to story37-topic.txt\n",
            "Saving story38.txt to story38.txt\n",
            "Saving story38-gold.txt to story38-gold.txt\n",
            "Saving story38-topic.txt to story38-topic.txt\n",
            "Saving story39.txt to story39.txt\n",
            "Saving story39-gold.txt to story39-gold.txt\n",
            "Saving story39-topic.txt to story39-topic.txt\n",
            "Saving story40.txt to story40.txt\n",
            "Saving story40-gold.txt to story40-gold.txt\n",
            "Saving story40-topic.txt to story40-topic.txt\n",
            "Saving story41.txt to story41.txt\n",
            "Saving story41-topic.txt to story41-topic.txt\n",
            "Saving story42.txt to story42.txt\n",
            "Saving story42-gold.txt to story42-gold.txt\n",
            "Saving story42-topic.txt to story42-topic.txt\n",
            "Saving story43.txt to story43.txt\n",
            "Saving story43-topic.txt to story43-topic.txt\n",
            "Saving story44.txt to story44.txt\n",
            "Saving story44-gold.txt to story44-gold.txt\n",
            "Saving story44-topic.txt to story44-topic.txt\n",
            "Saving story45.txt to story45.txt\n",
            "Saving story45-gold.txt to story45-gold.txt\n",
            "Saving story45-topic.txt to story45-topic.txt\n",
            "Saving story46.txt to story46.txt\n",
            "Saving story46-gold.txt to story46-gold.txt\n",
            "Saving story46-topic.txt to story46-topic.txt\n",
            "Saving story47.txt to story47.txt\n",
            "Saving story47-gold.txt to story47-gold.txt\n",
            "Saving story47-topic.txt to story47-topic.txt\n",
            "Saving story48.txt to story48.txt\n",
            "Saving story48-gold.txt to story48-gold.txt\n",
            "Saving story48-topic.txt to story48-topic.txt\n",
            "Saving story49.txt to story49.txt\n",
            "Saving story49-gold.txt to story49-gold.txt\n",
            "Saving story49-topic.txt to story49-topic.txt\n",
            "Saving story50.txt to story50.txt\n",
            "Saving story50-gold.txt to story50-gold.txt\n",
            "Saving story50-topic.txt to story50-topic.txt\n",
            "Saving story51.txt to story51.txt\n",
            "Saving story51-gold.txt to story51-gold.txt\n",
            "Saving story51-topic.txt to story51-topic.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7gUXWZMwb6D"
      },
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/guj_final_preporcess-v6.py\" \"guj_final_preporcess.py\" #The preprocessing modules designed\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/Evaluation.py\" \"Evaluation.py\"   # the evaluation module designed\n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/guj_final_postprocessing-v1.py\" \"post.py\"   # Get the post processing module \n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/my-dictionary-gujarati.csv\" \"my-dictionary-gujarati.csv\" #the static dictionary of words in gujarati\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo4oVUa5g9D-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5d5e7b-d6a1-4ef7-e8f6-75899ff2848b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import io\n",
        "import csv\n",
        "from Evaluation import *\n",
        "from post import *\n",
        "import pandas as pd \n",
        "from guj_final_preporcess import * #to get the required preporcessing modules -from self made pipeline\n",
        "import pyiwn"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25:11:56:14,3 INFO     [helpers.py:20] Downloading IndoWordNet data of size ~31 MB...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-25:11:56:15,529 INFO     [helpers.py:43] Extracting /root/iwn_data.tar.gz into /root...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-25:11:56:16,824 INFO     [helpers.py:48] Removing temporary zip file from /root/iwn_data.tar.gz\n",
            "2021-06-25:11:56:16,835 INFO     [helpers.py:51] IndoWordNet data successfully downloaded at /root/iwn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu3f_IRUd2NW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b5a68b-d587-44cd-c943-2d0aa2456624"
      },
      "source": [
        "#take all the needed inputs from user \n",
        "basename=input('Enter the basic name of file(without txt),other names wud be autogenerated  ')\n",
        "filename=basename+'.txt'\n",
        "topic_filename=basename+'-topic.txt'\n",
        "#filename=input('Enter text file name to be summarized ') #Ex: file1.txt\n",
        "analysefile_name=filename.replace('.txt','-analysis-Model1.txt')# to store the data for later analysis . Example- file1-results.txt \n",
        "features_filename=filename.replace('.txt','-Features-Model1.csv') # file-1-features.csv\n",
        "filename_statistics_all='datastatisticsall--Model1 .csv'\n",
        "#gold_filename=input(\"Enter name of file containing gold summary  \") #example file1-gold.txt ,assumes the name is having pattern ending with gold.txt \n",
        "summary_filename=filename.replace('gold.txt','-systemsummary--Model1 .txt') # example file1-gold-system-summary.txt\n",
        "\n",
        "\n",
        "rougescorefile=\"Rougeresults--Model1-- .csv\"\n",
        "dictionaryfile=\"my-dictionary-gujarati.csv\""
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the basic name of file(without txt),other names wud be autogenerated  story51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-yIPsZbj-Wn"
      },
      "source": [
        "\"\"\"\n",
        "print(\"enter modules you want to activate in current run \")\n",
        "vectormodel=input(\"1- Using TFIDF technique  2- Using TF technique :: your choice\")\n",
        "stopwordflag=input(\"3-With stopword removal phase    4- Without stopword removal phase:: your choice\")\n",
        "stemmerflag=input(\"5- With stemmer phase     6- Without stemmer phase:: your choice \")\n",
        "lemmaflag=input(\"7- With stem to lemma c3onversion.  8- Without stem to lemma conversion:: your choice\")\n",
        "cueflag=input(\"9- Using cuewords list 10-Without using cuewordslist\" )\n",
        "coreferenceflag=input(\"11- Using coreference resolution list 12-Without using coreference resolution\")\n",
        "\"\"\"\n",
        "vectormodel='2'\n",
        "stopwordflag='3'\n",
        "stemmerflag='5'\n",
        "lemmaflag='7'\n",
        "cueflag='9'\n",
        "coreferenceflag='11'\n",
        "\n"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZfHoaVCaViy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b92cb19-6b10-4c79-d6dc-2868ffbda70c"
      },
      "source": [
        "#read the input original file . \n",
        "\n",
        "guj_clean_html(filename,filename)# text cleaning remove the html tags and overwrite the original file \n",
        "\n",
        "guj_clean_html(topic_filename,topic_filename)\n",
        "\n",
        "guj_clean_extra(filename,filename)#remove the extra symbols like multiple punctuations and overwrite original file\n",
        "\n",
        "guj_clean_extra(topic_filename,topic_filename) \n",
        "#Display the given data in input files \n",
        "with io.open(filename,'r',encoding='utf-8') as f: \n",
        "\t\t  lines=f.read()#get all the lines from file\n",
        "analyzedfile_obj=open(analysefile_name,mode='w',encoding='UTF-8')\n",
        "analyzedfile_obj.write(\"\\n\\n The original file is :------\\n\")\n",
        "analyzedfile_obj.write(lines)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with io.open(topic_filename,'r',encoding='utf-8') as f: \n",
        "\t\t  lines=f.read()#get all the lines from file\n",
        "\n",
        "analyzedfile_obj.write(\"\\n\\n The topic source file given is :------\\n\")\n",
        "analyzedfile_obj.write(lines)\n",
        "\n",
        "\n",
        "\n",
        "# now find the statistics of originally given informations \n",
        "originalcorpora,totaloriginalsentences=guj_corpus_generate(filename) #make corpus of original data \n",
        "\n",
        "\n",
        "analyzedfile_obj.write(\"\\n\\n STATISTICS------>\")\n",
        "analyzedfile_obj.writelines(\"\\n Original file sentences(RAW) - {0}\\n\".format(totaloriginalsentences))\n",
        "\n",
        "\n",
        "###################################################################\n",
        "########################  Step-1 ##################################\n",
        "#####################  cleaning phase  ########################\n",
        "\n",
        "#doing cleaning of file \n",
        "#remove newlines and other unnecessary data and clean it for feature extraction \n",
        "#to replace the data as per our format, replacing abbreviations dot with '*'\n",
        "normfile=\"temp1.txt\"\n",
        "norm_gold_file=\"temp2.txt\"\n",
        "clean_topic_file=norm_topic_file=\"temp3.txt\"\n",
        "\n",
        "guj_clean_text(filename,normfile) \n",
        "\n",
        "\n",
        "guj_clean_text(topic_filename,clean_topic_file) #no further process needed for topic file, it is already cleaned.\n",
        "\n",
        "#Now display the stats and make corpora after cleaning file and removing new lines . \n",
        "normcorpora,totalnormsentences=guj_corpus_generate(normfile)  \n",
        "\n",
        "topiccorpora,totaltopicsentences=guj_corpus_generate(norm_topic_file)\n",
        "\n",
        "analyzedfile_obj.write(\" Segmented Lines in source -{0} \\n \".format(totalnormsentences))\n",
        "\n",
        "analyzedfile_obj.write(\" the total sentences in normalized topic file are= {0} \".format(totaltopicsentences))\n",
        "###################################################################\n",
        "########################  Step-2 ##################################\n",
        "#####################  Normalization and tokenization  for feature extraction  ########################\n",
        "\n",
        "cleanfile=\"file.txt\"\n",
        "stemmedlist=[] #to store the stemmed words \n",
        "lemmalist=[] #to store the lemmas(dictionary) words found from vacabulory \n",
        "mydictionary=guj_makelist(dictionaryfile) #generate the dictionary of unique words from given file of gujarati words\n",
        "vocab=guj_makevocab(normfile)\n",
        "\n",
        "words_before_stopword_removal=len(vocab)\n",
        "\n",
        "if (stopwordflag=='3'):\n",
        " cleanfile=guj_stopwordremoval(normfile,cleanfile) #remove stopwords done \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "else :\n",
        "  #print(\"stopwords are not removed\")\n",
        "  cleanfile=normfile # no stopword removal step done ,so clean file is directly the normalized one \n",
        "\n",
        "\n",
        "\n",
        "scorpora,stotalsent=guj_corpus_generate(cleanfile) #scorpora- the changed lines after stopword removal \n",
        "\n",
        "\n",
        "focus_list=guj_makelist(topic_filename)\n",
        "\n",
        "## logic for co-reference resolution --\n",
        "corpora,totalsent=guj_corpus_generate(cleanfile) #divide lines into sentences-\n",
        "vocab_topic=guj_makevocab(clean_topic_file) #extra words to be included in the feature extraction ,based on topic\n",
        "\n",
        "newlist=[]\n",
        "actorlist=list(vocab_topic.keys())\n",
        "\n",
        "stem_topic_filename=\"temp4.txt\"\n",
        "guj_stemmer_new(topic_filename,stem_topic_filename,mydictionary,actorlist)\n",
        "stemtopic,totalstemline=guj_corpus_generate(stem_topic_filename)\n",
        "extrawords=list(guj_makevocab(stem_topic_filename))\n",
        "for word in extrawords:\n",
        " actorlist.append(word)\n",
        "actorlist=set(actorlist) \n",
        "\n",
        "\n",
        "\n",
        "#Do co-reference resolution within previous 2 sentences\n",
        "if (coreferenceflag=='11'):\n",
        "  cleanfile=anaphoraresolution(corpora,actorlist,cleanfile)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab=guj_makevocab(cleanfile)\n",
        "words_before_stemming=len(vocab)#after anaphora resolution vocab size\n",
        "\n",
        "if (stemmerflag=='5'):\n",
        "  cleanfile,stemmedlist,lemmalist=guj_stemmer_new(cleanfile,cleanfile,mydictionary,focus_list) #stem with help from dictionary ,and get back two lists \n",
        "\n",
        "stemcorpora,stemtotalsent=guj_corpus_generate(cleanfile) #stemcorpora- the changed lines after stemming \n",
        "vocab=guj_makevocab(cleanfile)\n",
        "words_after_stemming=len(vocab)\n",
        "\n",
        "if (lemmaflag=='7'):\n",
        "  cleanfile=guj_stem_to_lemma(cleanfile,stemmedlist,lemmalist) #if have both stem and dictionary form of word ,convert to the dictionay form \n",
        "\n",
        "vocab=guj_makevocab(cleanfile)\n",
        "words_after_lemma=len(vocab)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "semgnet to make corpus for story51.txt\n",
            "corpus now has sentences --> \n",
            "118\n",
            "semgnet to make corpus for temp1.txt\n",
            "corpus now has sentences --> \n",
            "118\n",
            "semgnet to make corpus for temp3.txt\n",
            "corpus now has sentences --> \n",
            "1\n",
            "semgnet to make corpus for file.txt\n",
            "corpus now has sentences --> \n",
            "118\n",
            "semgnet to make corpus for file.txt\n",
            "corpus now has sentences --> \n",
            "118\n",
            "semgnet to make corpus for temp4.txt\n",
            "corpus now has sentences --> \n",
            "1\n",
            "semgnet to make corpus for file.txt\n",
            "corpus now has sentences --> \n",
            "118\n",
            "semgnet to make corpus for file.txt\n",
            "corpus now has sentences --> \n",
            "118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItWH7QJr9rFy",
        "outputId": "bd6834ed-23a3-4092-967b-5f36e84887e5"
      },
      "source": [
        "#!!!!!!!!!!!!!!!!!!!For finding the actors / Focus group in story---- using both tf and tfidf to extract featuers  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#####\n",
        "\n",
        "#Now working on the modified file to get various features \n",
        "#guj_clean_extra(finalfile,finalfile) #clean any residual marks in file \n",
        "#guj_clean_extra(preprocessedtopicfile,preprocessedtopicfile)\n",
        "\n",
        "\n",
        "corpora,totalsent=guj_corpus_generate(cleanfile) #divide lines into sentences-\n",
        "#print(\"\\n Lines in source after all processing -{0} \\n \".format(totalsent))\n",
        "sent_pos_list=[] #to store the sentence position\n",
        "sent_words_count=[]  # to store the total words in each sentence \n",
        "sent_number=0\n",
        "for s in corpora:\n",
        "  sent_pos_list.append(sent_number)\n",
        "  sent_number+=1\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "# to generate vector represnetations  -----> TF \n",
        "\n",
        "vectorizer_tf = CountVectorizer(tokenizer = guj_mytokenizer,min_df=0.15,max_df=0.85) #frequency of words with atleast 15% frequency\n",
        " \n",
        "\n",
        "try:\n",
        " cwm_tf = vectorizer_tf.fit_transform(corpora) #vectorize the corpus of input file\n",
        "  \n",
        " n=0\n",
        " top_features_tf = vectorizer_tf.vocabulary_ #to store the features extracted\n",
        " \n",
        " for f in top_features_tf.keys():\n",
        "   n+=1 #top features total count\n",
        " print(\"\\ntop features and their values were{0} -TF\".format(top_features_tf))\n",
        " pos=n\n",
        " tokens = vectorizer_tf.get_feature_names() #get the names of words used in vector   \n",
        "except: #no features able to pass the criteria \n",
        "  pos=0\n",
        "  top_features_tf={}\n",
        "\n",
        "\n",
        "#### Extra logic for topic based vectorizor feature upadation ###########333\n",
        "if (cueflag=='9'):\n",
        "\n",
        " for feature, index in vocab_topic.items(): #take all topic words from vocab and add in features \n",
        "    \n",
        "    if feature not in top_features_tf.keys() or len(top_features_tf)==0:#add only if not already considered in top words or no words already in top\n",
        "          top_features_tf[feature] = pos\n",
        "          pos=pos+1\n",
        " # re-vectorize using both sets of features\n",
        " \n",
        " vectorizer_tf = CountVectorizer(vocabulary=top_features_tf,tokenizer = guj_mytokenizer)   \n",
        " cwm_tf = vectorizer_tf.fit_transform(corpora)\n",
        " tokens_tf = vectorizer_tf.get_feature_names() #get the names of words used in vector\n",
        " top_features_tf = vectorizer_tf.vocabulary_\n",
        " print(\"\\n New top features after considering the actor words are {0} -TF\".format(top_features_tf))\n",
        "#### End of extra logic for adding separate topic based keywords in learning \n",
        "\n",
        "score_list_tf=[]\n",
        "counts_tf=cwm_tf.sum(axis=1) #sums the sentence wise words' tfidf scores\n",
        "scorelist_tf=list(counts_tf.flat)#get the list of sentencescores \n",
        "\n",
        "\n",
        "print(\"\\n\\n****The TF  based vector is formed ****\")\n",
        "print('\\n The top features used in vector are -\\n {0}-TF '.format(cwm_tf.shape))\n",
        "\n",
        "#### similar steps for tfidf scores computation###\n",
        " ##############################################################\n",
        " #############################################################\n",
        "# to generate vector represnetations  \n",
        "vectorizer_tfidf = TfidfVectorizer(tokenizer = guj_mytokenizer,min_df=0.10,max_df=0.85) #frequency of words with atleast 15% frequency\n",
        "\n",
        "try:\n",
        " cwm_tfidf = vectorizer_tfidf.fit_transform(corpora) #vectorize the corpus of input file\n",
        " \n",
        " n=0\n",
        " top_features_tfidf = vectorizer_tfidf.vocabulary_ #to store the features extracted\n",
        " \n",
        " for f in top_features_tfidf.keys():\n",
        "   n+=1 #top features total count\n",
        " print(\"\\ntop features and their values were{0} -\".format(top_features_tfidf))\n",
        " pos=n\n",
        " tokens = vectorizer_tfidf.get_feature_names() #get the names of words used in vector   \n",
        "except: #no features able to pass the criteria \n",
        "  pos=0\n",
        "  top_features_tfidf={}\n",
        "if (cueflag=='9'):\n",
        "\n",
        " for feature, index in vocab_topic.items(): #take all topic words from vocab and add in features \n",
        "    \n",
        "    if feature not in top_features_tfidf.keys() or len(top_features_tfidf)==0:#add only if not already considered in top words or no words already in top\n",
        "          top_features_tfidf[feature] = pos\n",
        "          pos=pos+1\n",
        " # re-vectorize using both sets of features\n",
        " vectorizer_tfidf = TfidfVectorizer(vocabulary=top_features_tfidf,tokenizer = guj_mytokenizer)\n",
        "  \n",
        " cwm_tfidf = vectorizer_tfidf.fit_transform(corpora)\n",
        " tokens_tfidf = vectorizer_tfidf.get_feature_names() #get the names of words used in vector\n",
        " top_features_tfidf = vectorizer_tfidf.vocabulary_\n",
        " print(\"\\n New top features after considering the actor words are {0} in TFIDF-\".format(top_features_tfidf))\n",
        "#### End of extra logic for adding separate topic based keywords in learning \n",
        "\n",
        "score_list_tfidf=[]\n",
        "counts_tfidf=cwm_tfidf.sum(axis=1) #sums the sentence wise words' tfidf scores\n",
        "scorelist_tfidf=list(counts_tfidf.flat)#get the list of sentencescores \n",
        "\n",
        "print(\"\\n\\n****The TFIDF  based vector is formed ****\")\n",
        "print('\\n The top features used in vector are -\\n {0} -TFIDF'.format(cwm_tfidf.shape))\n",
        "\n",
        "########################################################## \n",
        "################# ######Step3#############################\n",
        "################ Compute sentence scores #################\n",
        "##########################################################\n",
        "\n",
        "totwords=[]\n",
        "\n",
        "for s in corpora:\n",
        "   count=0\n",
        "   words=guj_mytokenizer(s)\n",
        "   for c in words:\n",
        "    count=count+1\n",
        "   totwords.append(count)\n",
        "\n",
        "\n",
        "list_of_tuples = list(zip(sent_pos_list,normcorpora,cwm_tf.toarray(),cwm_tfidf.toarray(),scorelist_tf,scorelist_tfidf,totwords))  \n",
        "df=pd.DataFrame(list_of_tuples, columns=['sent_pos','source_lines','feature_TF','feature_TFIDF','totalsentcount_tf','totalsentcount_tfidf','totalwordsinsent'])\n",
        "\n",
        "df['sent_score_tf']=df['totalsentcount_tf']/df['totalwordsinsent']\n",
        "df['sent_score_tfidf']=df['totalsentcount_tfidf']/df['totalwordsinsent']\n",
        "#based on sentence position , increase weightage of first and last and second last last sentences in txt \n",
        "\n",
        "\n",
        "df['bordersentences']='0' # Is current sentence start or end sentence in text? Default is no - 0 \n",
        "df.at[0,'bordersentences']='1'\n",
        "df.at[totalnormsentences-1,'bordersentences']='1'\n",
        "\n",
        "\n",
        "df.to_csv(features_filename) \n",
        "df.to_csv(filename_statistics_all,mode='a',header=False) \n"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "semgnet to make corpus for file.txt\n",
            "corpus now has sentences --> \n",
            "118\n",
            "\n",
            "top features and their values were{'સાહેબ': 1, 'નાથા': 0} -TF\n",
            "\n",
            " New top features after considering the actor words are {'સાહેબ': 1, 'નાથા': 0, 'મુખી': 2, 'બુધિયા': 3, 'સવિતા': 4} -TF\n",
            "\n",
            "\n",
            "****The TF  based vector is formed ****\n",
            "\n",
            " The top features used in vector are -\n",
            " (118, 5)-TF \n",
            "\n",
            "top features and their values were{'સાહેબ': 4, 'ઘર': 1, 'આવતા': 0, 'સવિતા': 3, 'નાથા': 2} -\n",
            "\n",
            " New top features after considering the actor words are {'સાહેબ': 4, 'ઘર': 1, 'આવતા': 0, 'સવિતા': 3, 'નાથા': 2, 'મુખી': 5, 'બુધિયા': 6} in TFIDF-\n",
            "\n",
            "\n",
            "****The TFIDF  based vector is formed ****\n",
            "\n",
            " The top features used in vector are -\n",
            " (118, 7) -TFIDF\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wix3MhE06QRs"
      },
      "source": [
        "analyzedfile_obj.close()\n",
        "\n"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIkTziH05hLV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc1ec390-66f4-438d-cb8f-0e503f69a8e4"
      },
      "source": [
        "## code to download all data at end of execution .Uncomment it after all experiments done. \n",
        "'''\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/txtfiles.zip *.txt\n",
        "files.download(\"txtfiles.zip\")\n",
        "!zip -r /content/csvfiles.zip *.csv\n",
        "files.download(\"csvfiles.zip\")\n",
        "!zip -r /content/codesfiles.zip *.py\n",
        "files.download(\"codesfiles.zip\")\n",
        "'''\n"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: file.txt (deflated 75%)\n",
            "  adding: story10-analysis-Model1.txt (deflated 71%)\n",
            "  adding: story10-gold.txt (deflated 71%)\n",
            "  adding: story10-topic.txt (deflated 11%)\n",
            "  adding: story10.txt (deflated 72%)\n",
            "  adding: story11-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story11-gold.txt (deflated 75%)\n",
            "  adding: story11-topic.txt (stored 0%)\n",
            "  adding: story11.txt (deflated 76%)\n",
            "  adding: story12-analysis-Model1.txt (deflated 76%)\n",
            "  adding: story12-gold.txt (deflated 78%)\n",
            "  adding: story12-topic.txt (deflated 2%)\n",
            "  adding: story12.txt (deflated 78%)\n",
            "  adding: story13-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story13-gold.txt (deflated 76%)\n",
            "  adding: story13-topic.txt (stored 0%)\n",
            "  adding: story13.txt (deflated 77%)\n",
            "  adding: story14-analysis-Model1.txt (deflated 76%)\n",
            "  adding: story14-gold.txt (deflated 77%)\n",
            "  adding: story14-topic.txt (deflated 24%)\n",
            "  adding: story14.txt (deflated 78%)\n",
            "  adding: story15-analysis-Model1.txt (deflated 73%)\n",
            "  adding: story15-gold.txt (deflated 73%)\n",
            "  adding: story15-topic.txt (deflated 18%)\n",
            "  adding: story15.txt (deflated 75%)\n",
            "  adding: story16-analysis-Model1.txt (deflated 79%)\n",
            "  adding: story16-gold.txt (deflated 80%)\n",
            "  adding: story16-topic.txt (deflated 41%)\n",
            "  adding: story16.txt (deflated 82%)\n",
            "  adding: story17-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story17-gold.txt (deflated 74%)\n",
            "  adding: story17-topic.txt (deflated 28%)\n",
            "  adding: story17.txt (deflated 75%)\n",
            "  adding: story18-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story18-gold.txt (deflated 73%)\n",
            "  adding: story18-topic.txt (deflated 22%)\n",
            "  adding: story18.txt (deflated 76%)\n",
            "  adding: story19-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story19-topic.txt (deflated 25%)\n",
            "  adding: story19.txt (deflated 75%)\n",
            "  adding: story1-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story1-gold.txt (deflated 73%)\n",
            "  adding: story1-topic.txt (stored 0%)\n",
            "  adding: story1.txt (deflated 76%)\n",
            "  adding: story20-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story20-gold.txt (deflated 75%)\n",
            "  adding: story20-topic.txt (deflated 32%)\n",
            "  adding: story20.txt (deflated 77%)\n",
            "  adding: story21-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story21-gold.txt (deflated 75%)\n",
            "  adding: story21-topic.txt (deflated 27%)\n",
            "  adding: story21.txt (deflated 76%)\n",
            "  adding: story22-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story22-gold.txt (deflated 74%)\n",
            "  adding: story22-topic.txt (deflated 5%)\n",
            "  adding: story22.txt (deflated 78%)\n",
            "  adding: story23-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story23-gold.txt (deflated 73%)\n",
            "  adding: story23-topic.txt (deflated 22%)\n",
            "  adding: story23.txt (deflated 77%)\n",
            "  adding: story24-analysis-Model1.txt (deflated 73%)\n",
            "  adding: story24-gold.txt (deflated 74%)\n",
            "  adding: story24-topic.txt (deflated 6%)\n",
            "  adding: story24.txt (deflated 75%)\n",
            "  adding: story25-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story25-gold.txt (deflated 75%)\n",
            "  adding: story25-topic.txt (stored 0%)\n",
            "  adding: story25.txt (deflated 77%)\n",
            "  adding: story26-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story26-gold.txt (deflated 76%)\n",
            "  adding: story26-topic.txt (stored 0%)\n",
            "  adding: story26.txt (deflated 78%)\n",
            "  adding: story27-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story27-gold.txt (deflated 75%)\n",
            "  adding: story27-topic.txt (deflated 23%)\n",
            "  adding: story27.txt (deflated 76%)\n",
            "  adding: story28-analysis-Model1.txt (deflated 71%)\n",
            "  adding: story28-gold.txt (deflated 73%)\n",
            "  adding: story28-topic.txt (stored 0%)\n",
            "  adding: story28.txt (deflated 74%)\n",
            "  adding: story29-analysis-Model1.txt (deflated 72%)\n",
            "  adding: story29-gold.txt (deflated 72%)\n",
            "  adding: story29-topic.txt (deflated 4%)\n",
            "  adding: story29.txt (deflated 74%)\n",
            "  adding: story2-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story2-gold.txt (deflated 74%)\n",
            "  adding: story2-topic.txt (deflated 15%)\n",
            "  adding: story2.txt (deflated 76%)\n",
            "  adding: story30-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story30-gold.txt (deflated 71%)\n",
            "  adding: story30-topic.txt (deflated 2%)\n",
            "  adding: story30.txt (deflated 75%)\n",
            "  adding: story31-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story31-gold.txt (deflated 74%)\n",
            "  adding: story31-topic.txt (deflated 19%)\n",
            "  adding: story31.txt (deflated 77%)\n",
            "  adding: story32-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story32-gold.txt (deflated 74%)\n",
            "  adding: story32-topic.txt (deflated 41%)\n",
            "  adding: story32.txt (deflated 76%)\n",
            "  adding: story33-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story33-gold.txt (deflated 76%)\n",
            "  adding: story33-topic.txt (stored 0%)\n",
            "  adding: story33.txt (deflated 77%)\n",
            "  adding: story34-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story34-gold.txt (deflated 74%)\n",
            "  adding: story34-topic.txt (deflated 17%)\n",
            "  adding: story34.txt (deflated 75%)\n",
            "  adding: story35-analysis-Model1.txt (deflated 76%)\n",
            "  adding: story35-gold.txt (deflated 77%)\n",
            "  adding: story35-topic.txt (deflated 15%)\n",
            "  adding: story35.txt (deflated 78%)\n",
            "  adding: story36-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story36-gold.txt (deflated 73%)\n",
            "  adding: story36-topic.txt (deflated 9%)\n",
            "  adding: story36.txt (deflated 76%)\n",
            "  adding: story37-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story37-gold.txt (deflated 75%)\n",
            "  adding: story37-topic.txt (deflated 19%)\n",
            "  adding: story37.txt (deflated 76%)\n",
            "  adding: story38-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story38-gold.txt (deflated 74%)\n",
            "  adding: story38-topic.txt (deflated 20%)\n",
            "  adding: story38.txt (deflated 75%)\n",
            "  adding: story39-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story39-gold.txt (deflated 75%)\n",
            "  adding: story39-topic.txt (deflated 23%)\n",
            "  adding: story39.txt (deflated 76%)\n",
            "  adding: story3-analysis-Model1.txt (deflated 73%)\n",
            "  adding: story3-gold.txt (deflated 74%)\n",
            "  adding: story3-topic.txt (stored 0%)\n",
            "  adding: story3.txt (deflated 75%)\n",
            "  adding: story40-analysis-Model1.txt (deflated 81%)\n",
            "  adding: story40-gold.txt (deflated 81%)\n",
            "  adding: story40-topic.txt (deflated 10%)\n",
            "  adding: story40.txt (deflated 81%)\n",
            "  adding: story41-analysis-Model1.txt (deflated 73%)\n",
            "  adding: story41-topic.txt (deflated 20%)\n",
            "  adding: story41.txt (deflated 74%)\n",
            "  adding: story42-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story42-gold.txt (deflated 73%)\n",
            "  adding: story42-topic.txt (deflated 14%)\n",
            "  adding: story42.txt (deflated 75%)\n",
            "  adding: story43-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story43-topic.txt (deflated 16%)\n",
            "  adding: story43.txt (deflated 75%)\n",
            "  adding: story44-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story44-gold.txt (deflated 75%)\n",
            "  adding: story44-topic.txt (deflated 3%)\n",
            "  adding: story44.txt (deflated 75%)\n",
            "  adding: story45-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story45-gold.txt (deflated 74%)\n",
            "  adding: story45-topic.txt (deflated 22%)\n",
            "  adding: story45.txt (deflated 75%)\n",
            "  adding: story46-analysis-Model1.txt (deflated 76%)\n",
            "  adding: story46-gold.txt (deflated 75%)\n",
            "  adding: story46-topic.txt (deflated 33%)\n",
            "  adding: story46.txt (deflated 77%)\n",
            "  adding: story47-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story47-gold.txt (deflated 74%)\n",
            "  adding: story47-topic.txt (deflated 23%)\n",
            "  adding: story47.txt (deflated 75%)\n",
            "  adding: story48-analysis-Model1.txt (deflated 76%)\n",
            "  adding: story48-gold.txt (deflated 75%)\n",
            "  adding: story48-topic.txt (deflated 6%)\n",
            "  adding: story48.txt (deflated 76%)\n",
            "  adding: story49-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story49-gold.txt (deflated 73%)\n",
            "  adding: story49-topic.txt (deflated 15%)\n",
            "  adding: story49.txt (deflated 75%)\n",
            "  adding: story4-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story4-gold.txt (deflated 74%)\n",
            "  adding: story4-topic.txt (stored 0%)\n",
            "  adding: story4.txt (deflated 76%)\n",
            "  adding: story50-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story50-gold.txt (deflated 75%)\n",
            "  adding: story50-topic.txt (deflated 13%)\n",
            "  adding: story50.txt (deflated 76%)\n",
            "  adding: story51-analysis-Model1.txt (deflated 75%)\n",
            "  adding: story51-gold.txt (deflated 75%)\n",
            "  adding: story51-topic.txt (deflated 19%)\n",
            "  adding: story51.txt (deflated 76%)\n",
            "  adding: story5-analysis-Model1.txt (deflated 72%)\n",
            "  adding: story5-gold.txt (deflated 72%)\n",
            "  adding: story5-topic.txt (stored 0%)\n",
            "  adding: story5.txt (deflated 73%)\n",
            "  adding: story6-analysis-Model1.txt (deflated 71%)\n",
            "  adding: story6-gold.txt (deflated 71%)\n",
            "  adding: story6-topic.txt (deflated 4%)\n",
            "  adding: story6.txt (deflated 73%)\n",
            "  adding: story7-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story7-gold.txt (deflated 74%)\n",
            "  adding: story7-topic.txt (deflated 3%)\n",
            "  adding: story7.txt (deflated 76%)\n",
            "  adding: story8-analysis-Model1.txt (deflated 71%)\n",
            "  adding: story8-gold.txt (deflated 72%)\n",
            "  adding: story8-topic.txt (deflated 4%)\n",
            "  adding: story8.txt (deflated 73%)\n",
            "  adding: story9-analysis-Model1.txt (deflated 74%)\n",
            "  adding: story9-gold.txt (deflated 73%)\n",
            "  adding: story9-topic.txt (stored 0%)\n",
            "  adding: story9.txt (deflated 75%)\n",
            "  adding: temp1.txt (deflated 76%)\n",
            "  adding: temp3.txt (deflated 18%)\n",
            "  adding: temp4.txt (deflated 19%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3d3990d7-fad7-4579-9d3d-8cd3f9d917b5\", \"txtfiles.zip\", 510684)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  adding: datastatisticsall--Model1 .csv (deflated 79%)\n",
            "  adding: lemmatized.csv (deflated 86%)\n",
            "  adding: my-dictionary-gujarati.csv (deflated 79%)\n",
            "  adding: stemmingdata.csv (deflated 78%)\n",
            "  adding: story10-Features-Model1.csv (deflated 73%)\n",
            "  adding: story11-Features-Model1.csv (deflated 75%)\n",
            "  adding: story12-Features-Model1.csv (deflated 77%)\n",
            "  adding: story13-Features-Model1.csv (deflated 77%)\n",
            "  adding: story14-Features-Model1.csv (deflated 79%)\n",
            "  adding: story15-Features-Model1.csv (deflated 76%)\n",
            "  adding: story16-Features-Model1.csv (deflated 81%)\n",
            "  adding: story17-Features-Model1.csv (deflated 77%)\n",
            "  adding: story18-Features-Model1.csv (deflated 78%)\n",
            "  adding: story19-Features-Model1.csv (deflated 78%)\n",
            "  adding: story1-Features-Model1.csv (deflated 76%)\n",
            "  adding: story20-Features-Model1.csv (deflated 77%)\n",
            "  adding: story21-Features-Model1.csv (deflated 78%)\n",
            "  adding: story22-Features-Model1.csv (deflated 77%)\n",
            "  adding: story23-Features-Model1.csv (deflated 78%)\n",
            "  adding: story24-Features-Model1.csv (deflated 79%)\n",
            "  adding: story25-Features-Model1.csv (deflated 76%)\n",
            "  adding: story26-Features-Model1.csv (deflated 77%)\n",
            "  adding: story27-Features-Model1.csv (deflated 76%)\n",
            "  adding: story28-Features-Model1.csv (deflated 77%)\n",
            "  adding: story29-Features-Model1.csv (deflated 74%)\n",
            "  adding: story2-Features-Model1.csv (deflated 76%)\n",
            "  adding: story30-Features-Model1.csv (deflated 75%)\n",
            "  adding: story31-Features-Model1.csv (deflated 76%)\n",
            "  adding: story32-Features-Model1.csv (deflated 78%)\n",
            "  adding: story33-Features-Model1.csv (deflated 75%)\n",
            "  adding: story34-Features-Model1.csv (deflated 79%)\n",
            "  adding: story35-Features-Model1.csv (deflated 79%)\n",
            "  adding: story36-Features-Model1.csv (deflated 77%)\n",
            "  adding: story37-Features-Model1.csv (deflated 77%)\n",
            "  adding: story38-Features-Model1.csv (deflated 79%)\n",
            "  adding: story39-Features-Model1.csv (deflated 78%)\n",
            "  adding: story3-Features-Model1.csv (deflated 75%)\n",
            "  adding: story40-Features-Model1.csv (deflated 79%)\n",
            "  adding: story41-Features-Model1.csv (deflated 76%)\n",
            "  adding: story42-Features-Model1.csv (deflated 77%)\n",
            "  adding: story43-Features-Model1.csv (deflated 76%)\n",
            "  adding: story44-Features-Model1.csv (deflated 77%)\n",
            "  adding: story45-Features-Model1.csv (deflated 79%)\n",
            "  adding: story46-Features-Model1.csv (deflated 79%)\n",
            "  adding: story47-Features-Model1.csv (deflated 79%)\n",
            "  adding: story48-Features-Model1.csv (deflated 77%)\n",
            "  adding: story49-Features-Model1.csv (deflated 79%)\n",
            "  adding: story4-Features-Model1.csv (deflated 76%)\n",
            "  adding: story50-Features-Model1.csv (deflated 77%)\n",
            "  adding: story51-Features-Model1.csv (deflated 77%)\n",
            "  adding: story5-Features-Model1.csv (deflated 74%)\n",
            "  adding: story6-Features-Model1.csv (deflated 74%)\n",
            "  adding: story7-Features-Model1.csv (deflated 75%)\n",
            "  adding: story8-Features-Model1.csv (deflated 76%)\n",
            "  adding: story9-Features-Model1.csv (deflated 73%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_41b57ce0-0339-4982-871f-b53af10900f0\", \"csvfiles.zip\", 808139)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  adding: Evaluation.py (deflated 64%)\n",
            "  adding: guj_final_preporcess.py (deflated 75%)\n",
            "  adding: post.py (deflated 70%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4dfb5b4a-4f94-4653-bb3f-35945ca238c8\", \"codesfiles.zip\", 12781)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}